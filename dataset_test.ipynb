{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import MixedImgDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 42\n",
      "Classes: ['Disturbance', 'IF', 'Normal']\n",
      "Class to index mapping: {'Disturbance': 0, 'IF': 1, 'Normal': 2}\n"
     ]
    }
   ],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "dataset = MixedImgDataset(root_dir='./data/MixedImg', transform=transform)\n",
    "\n",
    "# Check dataset\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Classes: {dataset.classes}\")\n",
    "print(f\"Class to index mapping: {dataset.class_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 42\n",
      "Training set size: 33\n",
      "Testing set size: 9\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "batch_size = 4\n",
    "# Define split ratio (e.g., 80% training, 20% testing)\n",
    "train_ratio = 0.8\n",
    "test_ratio = 1 - train_ratio\n",
    "\n",
    "# Calculate lengths\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create separate dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Print information about the splits\n",
    "print(f\"Total dataset size: {dataset_size}\")\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Testing set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18 output dimension: 3\n",
      "Number of parameters: 11178051\n",
      "layer3: Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "layer4: Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "avgpool: AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "fc: Linear(in_features=512, out_features=3, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangyu/.conda/envs/PyTc/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/xiangyu/.conda/envs/PyTc/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from models import BYOL\n",
    "from torch import nn\n",
    "\n",
    "# base_net = nn.Sequential(\n",
    "#         nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(64),\n",
    "#         nn.ReLU(),\n",
    "#         nn.AdaptiveAvgPool2d((1, 1)),\n",
    "#         nn.Flatten(),\n",
    "#         nn.Linear(64, 128),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(128, 128),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(128, 64),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(64, 32),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(32, 3),\n",
    "#     )\n",
    "\n",
    "\n",
    "# Initialize ResNet18\n",
    "base_net = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the final layer to output 3 classes\n",
    "num_features = base_net.fc.in_features\n",
    "base_net.fc = nn.Linear(num_features, 3)\n",
    "\n",
    "# Print model structure to verify\n",
    "print(f\"ResNet18 output dimension: {base_net.fc.out_features}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in base_net.parameters())}\")\n",
    "\n",
    "# Visualize the architecture of the last few layers\n",
    "for name, module in list(base_net.named_children())[-4:]:\n",
    "    print(f\"{name}: {module}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Step [1/9], Loss: 0.0119\n",
      "Epoch [1/200], Step [2/9], Loss: 0.0139\n",
      "Epoch [1/200], Step [3/9], Loss: 0.0130\n",
      "Epoch [1/200], Step [4/9], Loss: 0.0094\n",
      "Epoch [1/200], Step [5/9], Loss: 0.0127\n",
      "Epoch [1/200], Step [6/9], Loss: 0.0138\n",
      "Epoch [1/200], Step [7/9], Loss: 0.0137\n",
      "Epoch [1/200], Step [8/9], Loss: 0.0130\n",
      "Epoch [2/200], Step [1/9], Loss: 0.0139\n",
      "Epoch [2/200], Step [2/9], Loss: 0.0113\n",
      "Epoch [2/200], Step [3/9], Loss: 0.0113\n",
      "Epoch [2/200], Step [4/9], Loss: 0.0099\n",
      "Epoch [2/200], Step [5/9], Loss: 0.0109\n",
      "Epoch [2/200], Step [6/9], Loss: 0.0095\n",
      "Epoch [2/200], Step [7/9], Loss: 0.0097\n",
      "Epoch [2/200], Step [8/9], Loss: 0.0087\n",
      "Epoch [3/200], Step [1/9], Loss: 0.0108\n",
      "Epoch [3/200], Step [2/9], Loss: 0.0098\n",
      "Epoch [3/200], Step [3/9], Loss: 0.0084\n",
      "Epoch [3/200], Step [4/9], Loss: 0.0098\n",
      "Epoch [3/200], Step [5/9], Loss: 0.0076\n",
      "Epoch [3/200], Step [6/9], Loss: 0.0098\n",
      "Epoch [3/200], Step [7/9], Loss: 0.0089\n",
      "Epoch [3/200], Step [8/9], Loss: 0.0091\n",
      "Epoch [4/200], Step [1/9], Loss: 0.0091\n",
      "Epoch [4/200], Step [2/9], Loss: 0.0087\n",
      "Epoch [4/200], Step [3/9], Loss: 0.0080\n",
      "Epoch [4/200], Step [4/9], Loss: 0.0093\n",
      "Epoch [4/200], Step [5/9], Loss: 0.0090\n",
      "Epoch [4/200], Step [6/9], Loss: 0.0099\n",
      "Epoch [4/200], Step [7/9], Loss: 0.0077\n",
      "Epoch [4/200], Step [8/9], Loss: 0.0096\n",
      "Epoch [5/200], Step [1/9], Loss: 0.0084\n",
      "Epoch [5/200], Step [2/9], Loss: 0.0098\n",
      "Epoch [5/200], Step [3/9], Loss: 0.0087\n",
      "Epoch [5/200], Step [4/9], Loss: 0.0095\n",
      "Epoch [5/200], Step [5/9], Loss: 0.0088\n",
      "Epoch [5/200], Step [6/9], Loss: 0.0082\n",
      "Epoch [5/200], Step [7/9], Loss: 0.0083\n",
      "Epoch [5/200], Step [8/9], Loss: 0.0089\n",
      "Epoch [6/200], Step [1/9], Loss: 0.0089\n",
      "Epoch [6/200], Step [2/9], Loss: 0.0074\n",
      "Epoch [6/200], Step [3/9], Loss: 0.0098\n",
      "Epoch [6/200], Step [4/9], Loss: 0.0086\n",
      "Epoch [6/200], Step [5/9], Loss: 0.0101\n",
      "Epoch [6/200], Step [6/9], Loss: 0.0083\n",
      "Epoch [6/200], Step [7/9], Loss: 0.0076\n",
      "Epoch [6/200], Step [8/9], Loss: 0.0086\n",
      "Epoch [7/200], Step [1/9], Loss: 0.0103\n",
      "Epoch [7/200], Step [2/9], Loss: 0.0084\n",
      "Epoch [7/200], Step [3/9], Loss: 0.0079\n",
      "Epoch [7/200], Step [4/9], Loss: 0.0080\n",
      "Epoch [7/200], Step [5/9], Loss: 0.0081\n",
      "Epoch [7/200], Step [6/9], Loss: 0.0074\n",
      "Epoch [7/200], Step [7/9], Loss: 0.0122\n",
      "Epoch [7/200], Step [8/9], Loss: 0.0114\n",
      "Epoch [8/200], Step [1/9], Loss: 0.0085\n",
      "Epoch [8/200], Step [2/9], Loss: 0.0084\n",
      "Epoch [8/200], Step [3/9], Loss: 0.0116\n",
      "Epoch [8/200], Step [4/9], Loss: 0.0080\n",
      "Epoch [8/200], Step [5/9], Loss: 0.0077\n",
      "Epoch [8/200], Step [6/9], Loss: 0.0079\n",
      "Epoch [8/200], Step [7/9], Loss: 0.0086\n",
      "Epoch [8/200], Step [8/9], Loss: 0.0078\n",
      "Epoch [9/200], Step [1/9], Loss: 0.0083\n",
      "Epoch [9/200], Step [2/9], Loss: 0.0075\n",
      "Epoch [9/200], Step [3/9], Loss: 0.0080\n",
      "Epoch [9/200], Step [4/9], Loss: 0.0085\n",
      "Epoch [9/200], Step [5/9], Loss: 0.0085\n",
      "Epoch [9/200], Step [6/9], Loss: 0.0086\n",
      "Epoch [9/200], Step [7/9], Loss: 0.0083\n",
      "Epoch [9/200], Step [8/9], Loss: 0.0103\n",
      "Epoch [10/200], Step [1/9], Loss: 0.0077\n",
      "Epoch [10/200], Step [2/9], Loss: 0.0078\n",
      "Epoch [10/200], Step [3/9], Loss: 0.0088\n",
      "Epoch [10/200], Step [4/9], Loss: 0.0069\n",
      "Epoch [10/200], Step [5/9], Loss: 0.0082\n",
      "Epoch [10/200], Step [6/9], Loss: 0.0080\n",
      "Epoch [10/200], Step [7/9], Loss: 0.0083\n",
      "Epoch [10/200], Step [8/9], Loss: 0.0072\n",
      "Epoch [11/200], Step [1/9], Loss: 0.0077\n",
      "Epoch [11/200], Step [2/9], Loss: 0.0072\n",
      "Epoch [11/200], Step [3/9], Loss: 0.0078\n",
      "Epoch [11/200], Step [4/9], Loss: 0.0085\n",
      "Epoch [11/200], Step [5/9], Loss: 0.0080\n",
      "Epoch [11/200], Step [6/9], Loss: 0.0085\n",
      "Epoch [11/200], Step [7/9], Loss: 0.0081\n",
      "Epoch [11/200], Step [8/9], Loss: 0.0078\n",
      "Epoch [12/200], Step [1/9], Loss: 0.0077\n",
      "Epoch [12/200], Step [2/9], Loss: 0.0074\n",
      "Epoch [12/200], Step [3/9], Loss: 0.0071\n",
      "Epoch [12/200], Step [4/9], Loss: 0.0093\n",
      "Epoch [12/200], Step [5/9], Loss: 0.0094\n",
      "Epoch [12/200], Step [6/9], Loss: 0.0078\n",
      "Epoch [12/200], Step [7/9], Loss: 0.0078\n",
      "Epoch [12/200], Step [8/9], Loss: 0.0080\n",
      "Epoch [13/200], Step [1/9], Loss: 0.0082\n",
      "Epoch [13/200], Step [2/9], Loss: 0.0076\n",
      "Epoch [13/200], Step [3/9], Loss: 0.0079\n",
      "Epoch [13/200], Step [4/9], Loss: 0.0090\n",
      "Epoch [13/200], Step [5/9], Loss: 0.0074\n",
      "Epoch [13/200], Step [6/9], Loss: 0.0069\n",
      "Epoch [13/200], Step [7/9], Loss: 0.0078\n",
      "Epoch [13/200], Step [8/9], Loss: 0.0084\n",
      "Epoch [14/200], Step [1/9], Loss: 0.0078\n",
      "Epoch [14/200], Step [2/9], Loss: 0.0075\n",
      "Epoch [14/200], Step [3/9], Loss: 0.0076\n",
      "Epoch [14/200], Step [4/9], Loss: 0.0086\n",
      "Epoch [14/200], Step [5/9], Loss: 0.0076\n",
      "Epoch [14/200], Step [6/9], Loss: 0.0078\n",
      "Epoch [14/200], Step [7/9], Loss: 0.0082\n",
      "Epoch [14/200], Step [8/9], Loss: 0.0077\n",
      "Epoch [15/200], Step [1/9], Loss: 0.0072\n",
      "Epoch [15/200], Step [2/9], Loss: 0.0067\n",
      "Epoch [15/200], Step [3/9], Loss: 0.0076\n",
      "Epoch [15/200], Step [4/9], Loss: 0.0075\n",
      "Epoch [15/200], Step [5/9], Loss: 0.0072\n",
      "Epoch [15/200], Step [6/9], Loss: 0.0081\n",
      "Epoch [15/200], Step [7/9], Loss: 0.0071\n",
      "Epoch [15/200], Step [8/9], Loss: 0.0086\n",
      "Epoch [16/200], Step [1/9], Loss: 0.0070\n",
      "Epoch [16/200], Step [2/9], Loss: 0.0082\n",
      "Epoch [16/200], Step [3/9], Loss: 0.0081\n",
      "Epoch [16/200], Step [4/9], Loss: 0.0086\n",
      "Epoch [16/200], Step [5/9], Loss: 0.0069\n",
      "Epoch [16/200], Step [6/9], Loss: 0.0074\n",
      "Epoch [16/200], Step [7/9], Loss: 0.0078\n",
      "Epoch [16/200], Step [8/9], Loss: 0.0074\n",
      "Epoch [17/200], Step [1/9], Loss: 0.0070\n",
      "Epoch [17/200], Step [2/9], Loss: 0.0069\n",
      "Epoch [17/200], Step [3/9], Loss: 0.0077\n",
      "Epoch [17/200], Step [4/9], Loss: 0.0065\n",
      "Epoch [17/200], Step [5/9], Loss: 0.0065\n",
      "Epoch [17/200], Step [6/9], Loss: 0.0078\n",
      "Epoch [17/200], Step [7/9], Loss: 0.0082\n",
      "Epoch [17/200], Step [8/9], Loss: 0.0097\n",
      "Epoch [18/200], Step [1/9], Loss: 0.0070\n",
      "Epoch [18/200], Step [2/9], Loss: 0.0072\n",
      "Epoch [18/200], Step [3/9], Loss: 0.0069\n",
      "Epoch [18/200], Step [4/9], Loss: 0.0066\n",
      "Epoch [18/200], Step [5/9], Loss: 0.0081\n",
      "Epoch [18/200], Step [6/9], Loss: 0.0080\n",
      "Epoch [18/200], Step [7/9], Loss: 0.0075\n",
      "Epoch [18/200], Step [8/9], Loss: 0.0068\n",
      "Epoch [19/200], Step [1/9], Loss: 0.0073\n",
      "Epoch [19/200], Step [2/9], Loss: 0.0074\n",
      "Epoch [19/200], Step [3/9], Loss: 0.0078\n",
      "Epoch [19/200], Step [4/9], Loss: 0.0067\n",
      "Epoch [19/200], Step [5/9], Loss: 0.0062\n",
      "Epoch [19/200], Step [6/9], Loss: 0.0084\n",
      "Epoch [19/200], Step [7/9], Loss: 0.0073\n",
      "Epoch [19/200], Step [8/9], Loss: 0.0090\n",
      "Epoch [20/200], Step [1/9], Loss: 0.0086\n",
      "Epoch [20/200], Step [2/9], Loss: 0.0071\n",
      "Epoch [20/200], Step [3/9], Loss: 0.0073\n",
      "Epoch [20/200], Step [4/9], Loss: 0.0068\n",
      "Epoch [20/200], Step [5/9], Loss: 0.0061\n",
      "Epoch [20/200], Step [6/9], Loss: 0.0066\n",
      "Epoch [20/200], Step [7/9], Loss: 0.0100\n",
      "Epoch [20/200], Step [8/9], Loss: 0.0082\n",
      "Epoch [21/200], Step [1/9], Loss: 0.0071\n",
      "Epoch [21/200], Step [2/9], Loss: 0.0078\n",
      "Epoch [21/200], Step [3/9], Loss: 0.0073\n",
      "Epoch [21/200], Step [4/9], Loss: 0.0074\n",
      "Epoch [21/200], Step [5/9], Loss: 0.0079\n",
      "Epoch [21/200], Step [6/9], Loss: 0.0072\n",
      "Epoch [21/200], Step [7/9], Loss: 0.0071\n",
      "Epoch [21/200], Step [8/9], Loss: 0.0080\n",
      "Epoch [22/200], Step [1/9], Loss: 0.0078\n",
      "Epoch [22/200], Step [2/9], Loss: 0.0080\n",
      "Epoch [22/200], Step [3/9], Loss: 0.0104\n",
      "Epoch [22/200], Step [4/9], Loss: 0.0074\n",
      "Epoch [22/200], Step [5/9], Loss: 0.0067\n",
      "Epoch [22/200], Step [6/9], Loss: 0.0078\n",
      "Epoch [22/200], Step [7/9], Loss: 0.0069\n",
      "Epoch [22/200], Step [8/9], Loss: 0.0078\n",
      "Epoch [23/200], Step [1/9], Loss: 0.0075\n",
      "Epoch [23/200], Step [2/9], Loss: 0.0098\n",
      "Epoch [23/200], Step [3/9], Loss: 0.0077\n",
      "Epoch [23/200], Step [4/9], Loss: 0.0072\n",
      "Epoch [23/200], Step [5/9], Loss: 0.0075\n",
      "Epoch [23/200], Step [6/9], Loss: 0.0074\n",
      "Epoch [23/200], Step [7/9], Loss: 0.0068\n",
      "Epoch [23/200], Step [8/9], Loss: 0.0085\n",
      "Epoch [24/200], Step [1/9], Loss: 0.0073\n",
      "Epoch [24/200], Step [2/9], Loss: 0.0075\n",
      "Epoch [24/200], Step [3/9], Loss: 0.0101\n",
      "Epoch [24/200], Step [4/9], Loss: 0.0074\n",
      "Epoch [24/200], Step [5/9], Loss: 0.0068\n",
      "Epoch [24/200], Step [6/9], Loss: 0.0074\n",
      "Epoch [24/200], Step [7/9], Loss: 0.0071\n",
      "Epoch [24/200], Step [8/9], Loss: 0.0077\n",
      "Epoch [25/200], Step [1/9], Loss: 0.0068\n",
      "Epoch [25/200], Step [2/9], Loss: 0.0071\n",
      "Epoch [25/200], Step [3/9], Loss: 0.0076\n",
      "Epoch [25/200], Step [4/9], Loss: 0.0071\n",
      "Epoch [25/200], Step [5/9], Loss: 0.0079\n",
      "Epoch [25/200], Step [6/9], Loss: 0.0077\n",
      "Epoch [25/200], Step [7/9], Loss: 0.0072\n",
      "Epoch [25/200], Step [8/9], Loss: 0.0074\n",
      "Epoch [26/200], Step [1/9], Loss: 0.0070\n",
      "Epoch [26/200], Step [2/9], Loss: 0.0069\n",
      "Epoch [26/200], Step [3/9], Loss: 0.0072\n",
      "Epoch [26/200], Step [4/9], Loss: 0.0080\n",
      "Epoch [26/200], Step [5/9], Loss: 0.0129\n",
      "Epoch [26/200], Step [6/9], Loss: 0.0074\n",
      "Epoch [26/200], Step [7/9], Loss: 0.0066\n",
      "Epoch [26/200], Step [8/9], Loss: 0.0080\n",
      "Epoch [27/200], Step [1/9], Loss: 0.0072\n",
      "Epoch [27/200], Step [2/9], Loss: 0.0077\n",
      "Epoch [27/200], Step [3/9], Loss: 0.0081\n",
      "Epoch [27/200], Step [4/9], Loss: 0.0069\n",
      "Epoch [27/200], Step [5/9], Loss: 0.0083\n",
      "Epoch [27/200], Step [6/9], Loss: 0.0066\n",
      "Epoch [27/200], Step [7/9], Loss: 0.0069\n",
      "Epoch [27/200], Step [8/9], Loss: 0.0091\n",
      "Epoch [28/200], Step [1/9], Loss: 0.0060\n",
      "Epoch [28/200], Step [2/9], Loss: 0.0068\n",
      "Epoch [28/200], Step [3/9], Loss: 0.0074\n",
      "Epoch [28/200], Step [4/9], Loss: 0.0077\n",
      "Epoch [28/200], Step [5/9], Loss: 0.0070\n",
      "Epoch [28/200], Step [6/9], Loss: 0.0076\n",
      "Epoch [28/200], Step [7/9], Loss: 0.0068\n",
      "Epoch [28/200], Step [8/9], Loss: 0.0063\n",
      "Epoch [29/200], Step [1/9], Loss: 0.0067\n",
      "Epoch [29/200], Step [2/9], Loss: 0.0077\n",
      "Epoch [29/200], Step [3/9], Loss: 0.0072\n",
      "Epoch [29/200], Step [4/9], Loss: 0.0068\n",
      "Epoch [29/200], Step [5/9], Loss: 0.0079\n",
      "Epoch [29/200], Step [6/9], Loss: 0.0064\n",
      "Epoch [29/200], Step [7/9], Loss: 0.0076\n",
      "Epoch [29/200], Step [8/9], Loss: 0.0075\n",
      "Epoch [30/200], Step [1/9], Loss: 0.0071\n",
      "Epoch [30/200], Step [2/9], Loss: 0.0082\n",
      "Epoch [30/200], Step [3/9], Loss: 0.0087\n",
      "Epoch [30/200], Step [4/9], Loss: 0.0096\n",
      "Epoch [30/200], Step [5/9], Loss: 0.0070\n",
      "Epoch [30/200], Step [6/9], Loss: 0.0069\n",
      "Epoch [30/200], Step [7/9], Loss: 0.0073\n",
      "Epoch [30/200], Step [8/9], Loss: 0.0085\n",
      "Epoch [31/200], Step [1/9], Loss: 0.0070\n",
      "Epoch [31/200], Step [2/9], Loss: 0.0077\n",
      "Epoch [31/200], Step [3/9], Loss: 0.0076\n",
      "Epoch [31/200], Step [4/9], Loss: 0.0068\n",
      "Epoch [31/200], Step [5/9], Loss: 0.0066\n",
      "Epoch [31/200], Step [6/9], Loss: 0.0081\n",
      "Epoch [31/200], Step [7/9], Loss: 0.0136\n",
      "Epoch [31/200], Step [8/9], Loss: 0.0088\n",
      "Epoch [32/200], Step [1/9], Loss: 0.0069\n",
      "Epoch [32/200], Step [2/9], Loss: 0.0077\n",
      "Epoch [32/200], Step [3/9], Loss: 0.0067\n",
      "Epoch [32/200], Step [4/9], Loss: 0.0070\n",
      "Epoch [32/200], Step [5/9], Loss: 0.0078\n",
      "Epoch [32/200], Step [6/9], Loss: 0.0076\n",
      "Epoch [32/200], Step [7/9], Loss: 0.0068\n",
      "Epoch [32/200], Step [8/9], Loss: 0.0090\n",
      "Epoch [33/200], Step [1/9], Loss: 0.0072\n",
      "Epoch [33/200], Step [2/9], Loss: 0.0087\n",
      "Epoch [33/200], Step [3/9], Loss: 0.0078\n",
      "Epoch [33/200], Step [4/9], Loss: 0.0099\n",
      "Epoch [33/200], Step [5/9], Loss: 0.0071\n",
      "Epoch [33/200], Step [6/9], Loss: 0.0078\n",
      "Epoch [33/200], Step [7/9], Loss: 0.0067\n",
      "Epoch [33/200], Step [8/9], Loss: 0.0071\n",
      "Epoch [34/200], Step [1/9], Loss: 0.0087\n",
      "Epoch [34/200], Step [2/9], Loss: 0.0070\n",
      "Epoch [34/200], Step [3/9], Loss: 0.0073\n",
      "Epoch [34/200], Step [4/9], Loss: 0.0068\n",
      "Epoch [34/200], Step [5/9], Loss: 0.0081\n",
      "Epoch [34/200], Step [6/9], Loss: 0.0068\n",
      "Epoch [34/200], Step [7/9], Loss: 0.0073\n",
      "Epoch [34/200], Step [8/9], Loss: 0.0061\n",
      "Epoch [35/200], Step [1/9], Loss: 0.0081\n",
      "Epoch [35/200], Step [2/9], Loss: 0.0078\n",
      "Epoch [35/200], Step [3/9], Loss: 0.0067\n",
      "Epoch [35/200], Step [4/9], Loss: 0.0068\n",
      "Epoch [35/200], Step [5/9], Loss: 0.0081\n",
      "Epoch [35/200], Step [6/9], Loss: 0.0064\n",
      "Epoch [35/200], Step [7/9], Loss: 0.0070\n",
      "Epoch [35/200], Step [8/9], Loss: 0.0067\n",
      "Epoch [36/200], Step [1/9], Loss: 0.0074\n",
      "Epoch [36/200], Step [2/9], Loss: 0.0069\n",
      "Epoch [36/200], Step [3/9], Loss: 0.0085\n",
      "Epoch [36/200], Step [4/9], Loss: 0.0059\n",
      "Epoch [36/200], Step [5/9], Loss: 0.0061\n",
      "Epoch [36/200], Step [6/9], Loss: 0.0061\n",
      "Epoch [36/200], Step [7/9], Loss: 0.0095\n",
      "Epoch [36/200], Step [8/9], Loss: 0.0070\n",
      "Epoch [37/200], Step [1/9], Loss: 0.0066\n",
      "Epoch [37/200], Step [2/9], Loss: 0.0069\n",
      "Epoch [37/200], Step [3/9], Loss: 0.0075\n",
      "Epoch [37/200], Step [4/9], Loss: 0.0077\n",
      "Epoch [37/200], Step [5/9], Loss: 0.0071\n",
      "Epoch [37/200], Step [6/9], Loss: 0.0073\n",
      "Epoch [37/200], Step [7/9], Loss: 0.0080\n",
      "Epoch [37/200], Step [8/9], Loss: 0.0068\n",
      "Epoch [38/200], Step [1/9], Loss: 0.0066\n",
      "Epoch [38/200], Step [2/9], Loss: 0.0068\n",
      "Epoch [38/200], Step [3/9], Loss: 0.0080\n",
      "Epoch [38/200], Step [4/9], Loss: 0.0069\n",
      "Epoch [38/200], Step [5/9], Loss: 0.0075\n",
      "Epoch [38/200], Step [6/9], Loss: 0.0069\n",
      "Epoch [38/200], Step [7/9], Loss: 0.0071\n",
      "Epoch [38/200], Step [8/9], Loss: 0.0058\n",
      "Epoch [39/200], Step [1/9], Loss: 0.0083\n",
      "Epoch [39/200], Step [2/9], Loss: 0.0062\n",
      "Epoch [39/200], Step [3/9], Loss: 0.0070\n",
      "Epoch [39/200], Step [4/9], Loss: 0.0073\n",
      "Epoch [39/200], Step [5/9], Loss: 0.0082\n",
      "Epoch [39/200], Step [6/9], Loss: 0.0057\n",
      "Epoch [39/200], Step [7/9], Loss: 0.0091\n",
      "Epoch [39/200], Step [8/9], Loss: 0.0079\n",
      "Epoch [40/200], Step [1/9], Loss: 0.0068\n",
      "Epoch [40/200], Step [2/9], Loss: 0.0062\n",
      "Epoch [40/200], Step [3/9], Loss: 0.0075\n",
      "Epoch [40/200], Step [4/9], Loss: 0.0090\n",
      "Epoch [40/200], Step [5/9], Loss: 0.0073\n",
      "Epoch [40/200], Step [6/9], Loss: 0.0068\n",
      "Epoch [40/200], Step [7/9], Loss: 0.0079\n",
      "Epoch [40/200], Step [8/9], Loss: 0.0071\n",
      "Epoch [41/200], Step [1/9], Loss: 0.0087\n",
      "Epoch [41/200], Step [2/9], Loss: 0.0073\n",
      "Epoch [41/200], Step [3/9], Loss: 0.0080\n",
      "Epoch [41/200], Step [4/9], Loss: 0.0075\n",
      "Epoch [41/200], Step [5/9], Loss: 0.0070\n",
      "Epoch [41/200], Step [6/9], Loss: 0.0078\n",
      "Epoch [41/200], Step [7/9], Loss: 0.0062\n",
      "Epoch [41/200], Step [8/9], Loss: 0.0077\n",
      "Epoch [42/200], Step [1/9], Loss: 0.0071\n",
      "Epoch [42/200], Step [2/9], Loss: 0.0067\n",
      "Epoch [42/200], Step [3/9], Loss: 0.0075\n",
      "Epoch [42/200], Step [4/9], Loss: 0.0062\n",
      "Epoch [42/200], Step [5/9], Loss: 0.0101\n",
      "Epoch [42/200], Step [6/9], Loss: 0.0059\n",
      "Epoch [42/200], Step [7/9], Loss: 0.0059\n",
      "Epoch [42/200], Step [8/9], Loss: 0.0060\n",
      "Epoch [43/200], Step [1/9], Loss: 0.0067\n",
      "Epoch [43/200], Step [2/9], Loss: 0.0073\n",
      "Epoch [43/200], Step [3/9], Loss: 0.0071\n",
      "Epoch [43/200], Step [4/9], Loss: 0.0056\n",
      "Epoch [43/200], Step [5/9], Loss: 0.0078\n",
      "Epoch [43/200], Step [6/9], Loss: 0.0073\n",
      "Epoch [43/200], Step [7/9], Loss: 0.0076\n",
      "Epoch [43/200], Step [8/9], Loss: 0.0056\n",
      "Epoch [44/200], Step [1/9], Loss: 0.0059\n",
      "Epoch [44/200], Step [2/9], Loss: 0.0082\n",
      "Epoch [44/200], Step [3/9], Loss: 0.0066\n",
      "Epoch [44/200], Step [4/9], Loss: 0.0062\n",
      "Epoch [44/200], Step [5/9], Loss: 0.0072\n",
      "Epoch [44/200], Step [6/9], Loss: 0.0080\n",
      "Epoch [44/200], Step [7/9], Loss: 0.0075\n",
      "Epoch [44/200], Step [8/9], Loss: 0.0084\n",
      "Epoch [45/200], Step [1/9], Loss: 0.0077\n",
      "Epoch [45/200], Step [2/9], Loss: 0.0068\n",
      "Epoch [45/200], Step [3/9], Loss: 0.0067\n",
      "Epoch [45/200], Step [4/9], Loss: 0.0060\n",
      "Epoch [45/200], Step [5/9], Loss: 0.0075\n",
      "Epoch [45/200], Step [6/9], Loss: 0.0061\n",
      "Epoch [45/200], Step [7/9], Loss: 0.0072\n",
      "Epoch [45/200], Step [8/9], Loss: 0.0067\n",
      "Epoch [46/200], Step [1/9], Loss: 0.0082\n",
      "Epoch [46/200], Step [2/9], Loss: 0.0069\n",
      "Epoch [46/200], Step [3/9], Loss: 0.0079\n",
      "Epoch [46/200], Step [4/9], Loss: 0.0075\n",
      "Epoch [46/200], Step [5/9], Loss: 0.0072\n",
      "Epoch [46/200], Step [6/9], Loss: 0.0072\n",
      "Epoch [46/200], Step [7/9], Loss: 0.0074\n",
      "Epoch [46/200], Step [8/9], Loss: 0.0078\n",
      "Epoch [47/200], Step [1/9], Loss: 0.0081\n",
      "Epoch [47/200], Step [2/9], Loss: 0.0071\n",
      "Epoch [47/200], Step [3/9], Loss: 0.0057\n",
      "Epoch [47/200], Step [4/9], Loss: 0.0088\n",
      "Epoch [47/200], Step [5/9], Loss: 0.0074\n",
      "Epoch [47/200], Step [6/9], Loss: 0.0072\n",
      "Epoch [47/200], Step [7/9], Loss: 0.0072\n",
      "Epoch [47/200], Step [8/9], Loss: 0.0073\n",
      "Epoch [48/200], Step [1/9], Loss: 0.0070\n",
      "Epoch [48/200], Step [2/9], Loss: 0.0076\n",
      "Epoch [48/200], Step [3/9], Loss: 0.0077\n",
      "Epoch [48/200], Step [4/9], Loss: 0.0092\n",
      "Epoch [48/200], Step [5/9], Loss: 0.0065\n",
      "Epoch [48/200], Step [6/9], Loss: 0.0077\n",
      "Epoch [48/200], Step [7/9], Loss: 0.0070\n",
      "Epoch [48/200], Step [8/9], Loss: 0.0074\n",
      "Epoch [49/200], Step [1/9], Loss: 0.0067\n",
      "Epoch [49/200], Step [2/9], Loss: 0.0070\n",
      "Epoch [49/200], Step [3/9], Loss: 0.0063\n",
      "Epoch [49/200], Step [4/9], Loss: 0.0067\n",
      "Epoch [49/200], Step [5/9], Loss: 0.0082\n",
      "Epoch [49/200], Step [6/9], Loss: 0.0062\n",
      "Epoch [49/200], Step [7/9], Loss: 0.0066\n",
      "Epoch [49/200], Step [8/9], Loss: 0.0070\n",
      "Epoch [50/200], Step [1/9], Loss: 0.0066\n",
      "Epoch [50/200], Step [2/9], Loss: 0.0063\n",
      "Epoch [50/200], Step [3/9], Loss: 0.0068\n",
      "Epoch [50/200], Step [4/9], Loss: 0.0064\n",
      "Epoch [50/200], Step [5/9], Loss: 0.0079\n",
      "Epoch [50/200], Step [6/9], Loss: 0.0072\n",
      "Epoch [50/200], Step [7/9], Loss: 0.0063\n",
      "Epoch [50/200], Step [8/9], Loss: 0.0066\n",
      "Epoch [51/200], Step [1/9], Loss: 0.0073\n",
      "Epoch [51/200], Step [2/9], Loss: 0.0060\n",
      "Epoch [51/200], Step [3/9], Loss: 0.0067\n",
      "Epoch [51/200], Step [4/9], Loss: 0.0059\n",
      "Epoch [51/200], Step [5/9], Loss: 0.0062\n",
      "Epoch [51/200], Step [6/9], Loss: 0.0078\n",
      "Epoch [51/200], Step [7/9], Loss: 0.0055\n",
      "Epoch [51/200], Step [8/9], Loss: 0.0072\n",
      "Epoch [52/200], Step [1/9], Loss: 0.0077\n",
      "Epoch [52/200], Step [2/9], Loss: 0.0062\n",
      "Epoch [52/200], Step [3/9], Loss: 0.0060\n",
      "Epoch [52/200], Step [4/9], Loss: 0.0077\n",
      "Epoch [52/200], Step [5/9], Loss: 0.0071\n",
      "Epoch [52/200], Step [6/9], Loss: 0.0083\n",
      "Epoch [52/200], Step [7/9], Loss: 0.0089\n",
      "Epoch [52/200], Step [8/9], Loss: 0.0059\n",
      "Epoch [53/200], Step [1/9], Loss: 0.0072\n",
      "Epoch [53/200], Step [2/9], Loss: 0.0068\n",
      "Epoch [53/200], Step [3/9], Loss: 0.0074\n",
      "Epoch [53/200], Step [4/9], Loss: 0.0056\n",
      "Epoch [53/200], Step [5/9], Loss: 0.0070\n",
      "Epoch [53/200], Step [6/9], Loss: 0.0050\n",
      "Epoch [53/200], Step [7/9], Loss: 0.0075\n",
      "Epoch [53/200], Step [8/9], Loss: 0.0067\n",
      "Epoch [54/200], Step [1/9], Loss: 0.0067\n",
      "Epoch [54/200], Step [2/9], Loss: 0.0068\n",
      "Epoch [54/200], Step [3/9], Loss: 0.0067\n",
      "Epoch [54/200], Step [4/9], Loss: 0.0053\n",
      "Epoch [54/200], Step [5/9], Loss: 0.0074\n",
      "Epoch [54/200], Step [6/9], Loss: 0.0071\n",
      "Epoch [54/200], Step [7/9], Loss: 0.0076\n",
      "Epoch [54/200], Step [8/9], Loss: 0.0074\n",
      "Epoch [55/200], Step [1/9], Loss: 0.0080\n",
      "Epoch [55/200], Step [2/9], Loss: 0.0062\n",
      "Epoch [55/200], Step [3/9], Loss: 0.0067\n",
      "Epoch [55/200], Step [4/9], Loss: 0.0069\n",
      "Epoch [55/200], Step [5/9], Loss: 0.0062\n",
      "Epoch [55/200], Step [6/9], Loss: 0.0086\n",
      "Epoch [55/200], Step [7/9], Loss: 0.0064\n",
      "Epoch [55/200], Step [8/9], Loss: 0.0096\n",
      "Epoch [56/200], Step [1/9], Loss: 0.0073\n",
      "Epoch [56/200], Step [2/9], Loss: 0.0088\n",
      "Epoch [56/200], Step [3/9], Loss: 0.0063\n",
      "Epoch [56/200], Step [4/9], Loss: 0.0065\n",
      "Epoch [56/200], Step [5/9], Loss: 0.0067\n",
      "Epoch [56/200], Step [6/9], Loss: 0.0061\n",
      "Epoch [56/200], Step [7/9], Loss: 0.0079\n",
      "Epoch [56/200], Step [8/9], Loss: 0.0069\n",
      "Epoch [57/200], Step [1/9], Loss: 0.0070\n",
      "Epoch [57/200], Step [2/9], Loss: 0.0070\n",
      "Epoch [57/200], Step [3/9], Loss: 0.0052\n",
      "Epoch [57/200], Step [4/9], Loss: 0.0072\n",
      "Epoch [57/200], Step [5/9], Loss: 0.0076\n",
      "Epoch [57/200], Step [6/9], Loss: 0.0058\n",
      "Epoch [57/200], Step [7/9], Loss: 0.0080\n",
      "Epoch [57/200], Step [8/9], Loss: 0.0073\n",
      "Epoch [58/200], Step [1/9], Loss: 0.0054\n",
      "Epoch [58/200], Step [2/9], Loss: 0.0069\n",
      "Epoch [58/200], Step [3/9], Loss: 0.0075\n",
      "Epoch [58/200], Step [4/9], Loss: 0.0070\n",
      "Epoch [58/200], Step [5/9], Loss: 0.0084\n",
      "Epoch [58/200], Step [6/9], Loss: 0.0070\n",
      "Epoch [58/200], Step [7/9], Loss: 0.0082\n",
      "Epoch [58/200], Step [8/9], Loss: 0.0066\n",
      "Epoch [59/200], Step [1/9], Loss: 0.0064\n",
      "Epoch [59/200], Step [2/9], Loss: 0.0068\n",
      "Epoch [59/200], Step [3/9], Loss: 0.0083\n",
      "Epoch [59/200], Step [4/9], Loss: 0.0066\n",
      "Epoch [59/200], Step [5/9], Loss: 0.0061\n",
      "Epoch [59/200], Step [6/9], Loss: 0.0061\n",
      "Epoch [59/200], Step [7/9], Loss: 0.0069\n",
      "Epoch [59/200], Step [8/9], Loss: 0.0055\n",
      "Epoch [60/200], Step [1/9], Loss: 0.0072\n",
      "Epoch [60/200], Step [2/9], Loss: 0.0066\n",
      "Epoch [60/200], Step [3/9], Loss: 0.0057\n",
      "Epoch [60/200], Step [4/9], Loss: 0.0074\n",
      "Epoch [60/200], Step [5/9], Loss: 0.0054\n",
      "Epoch [60/200], Step [6/9], Loss: 0.0076\n",
      "Epoch [60/200], Step [7/9], Loss: 0.0061\n",
      "Epoch [60/200], Step [8/9], Loss: 0.0072\n",
      "Epoch [61/200], Step [1/9], Loss: 0.0065\n",
      "Epoch [61/200], Step [2/9], Loss: 0.0069\n",
      "Epoch [61/200], Step [3/9], Loss: 0.0056\n",
      "Epoch [61/200], Step [4/9], Loss: 0.0061\n",
      "Epoch [61/200], Step [5/9], Loss: 0.0059\n",
      "Epoch [61/200], Step [6/9], Loss: 0.0059\n",
      "Epoch [61/200], Step [7/9], Loss: 0.0063\n",
      "Epoch [61/200], Step [8/9], Loss: 0.0081\n",
      "Epoch [62/200], Step [1/9], Loss: 0.0064\n",
      "Epoch [62/200], Step [2/9], Loss: 0.0063\n",
      "Epoch [62/200], Step [3/9], Loss: 0.0073\n",
      "Epoch [62/200], Step [4/9], Loss: 0.0060\n",
      "Epoch [62/200], Step [5/9], Loss: 0.0075\n",
      "Epoch [62/200], Step [6/9], Loss: 0.0074\n",
      "Epoch [62/200], Step [7/9], Loss: 0.0054\n",
      "Epoch [62/200], Step [8/9], Loss: 0.0071\n",
      "Epoch [63/200], Step [1/9], Loss: 0.0070\n",
      "Epoch [63/200], Step [2/9], Loss: 0.0073\n",
      "Epoch [63/200], Step [3/9], Loss: 0.0056\n",
      "Epoch [63/200], Step [4/9], Loss: 0.0063\n",
      "Epoch [63/200], Step [5/9], Loss: 0.0085\n",
      "Epoch [63/200], Step [6/9], Loss: 0.0056\n",
      "Epoch [63/200], Step [7/9], Loss: 0.0067\n",
      "Epoch [63/200], Step [8/9], Loss: 0.0069\n",
      "Epoch [64/200], Step [1/9], Loss: 0.0070\n",
      "Epoch [64/200], Step [2/9], Loss: 0.0071\n",
      "Epoch [64/200], Step [3/9], Loss: 0.0069\n",
      "Epoch [64/200], Step [4/9], Loss: 0.0069\n",
      "Epoch [64/200], Step [5/9], Loss: 0.0075\n",
      "Epoch [64/200], Step [6/9], Loss: 0.0060\n",
      "Epoch [64/200], Step [7/9], Loss: 0.0060\n",
      "Epoch [64/200], Step [8/9], Loss: 0.0070\n",
      "Epoch [65/200], Step [1/9], Loss: 0.0079\n",
      "Epoch [65/200], Step [2/9], Loss: 0.0053\n",
      "Epoch [65/200], Step [3/9], Loss: 0.0065\n",
      "Epoch [65/200], Step [4/9], Loss: 0.0071\n",
      "Epoch [65/200], Step [5/9], Loss: 0.0053\n",
      "Epoch [65/200], Step [6/9], Loss: 0.0057\n",
      "Epoch [65/200], Step [7/9], Loss: 0.0078\n",
      "Epoch [65/200], Step [8/9], Loss: 0.0053\n",
      "Epoch [66/200], Step [1/9], Loss: 0.0064\n",
      "Epoch [66/200], Step [2/9], Loss: 0.0067\n",
      "Epoch [66/200], Step [3/9], Loss: 0.0065\n",
      "Epoch [66/200], Step [4/9], Loss: 0.0074\n",
      "Epoch [66/200], Step [5/9], Loss: 0.0063\n",
      "Epoch [66/200], Step [6/9], Loss: 0.0074\n",
      "Epoch [66/200], Step [7/9], Loss: 0.0074\n",
      "Epoch [66/200], Step [8/9], Loss: 0.0071\n",
      "Epoch [67/200], Step [1/9], Loss: 0.0069\n",
      "Epoch [67/200], Step [2/9], Loss: 0.0070\n",
      "Epoch [67/200], Step [3/9], Loss: 0.0070\n",
      "Epoch [67/200], Step [4/9], Loss: 0.0051\n",
      "Epoch [67/200], Step [5/9], Loss: 0.0071\n",
      "Epoch [67/200], Step [6/9], Loss: 0.0062\n",
      "Epoch [67/200], Step [7/9], Loss: 0.0080\n",
      "Epoch [67/200], Step [8/9], Loss: 0.0070\n",
      "Epoch [68/200], Step [1/9], Loss: 0.0063\n",
      "Epoch [68/200], Step [2/9], Loss: 0.0075\n",
      "Epoch [68/200], Step [3/9], Loss: 0.0057\n",
      "Epoch [68/200], Step [4/9], Loss: 0.0054\n",
      "Epoch [68/200], Step [5/9], Loss: 0.0078\n",
      "Epoch [68/200], Step [6/9], Loss: 0.0054\n",
      "Epoch [68/200], Step [7/9], Loss: 0.0081\n",
      "Epoch [68/200], Step [8/9], Loss: 0.0064\n",
      "Epoch [69/200], Step [1/9], Loss: 0.0062\n",
      "Epoch [69/200], Step [2/9], Loss: 0.0053\n",
      "Epoch [69/200], Step [3/9], Loss: 0.0054\n",
      "Epoch [69/200], Step [4/9], Loss: 0.0089\n",
      "Epoch [69/200], Step [5/9], Loss: 0.0072\n",
      "Epoch [69/200], Step [6/9], Loss: 0.0066\n",
      "Epoch [69/200], Step [7/9], Loss: 0.0070\n",
      "Epoch [69/200], Step [8/9], Loss: 0.0071\n",
      "Epoch [70/200], Step [1/9], Loss: 0.0050\n",
      "Epoch [70/200], Step [2/9], Loss: 0.0070\n",
      "Epoch [70/200], Step [3/9], Loss: 0.0067\n",
      "Epoch [70/200], Step [4/9], Loss: 0.0071\n",
      "Epoch [70/200], Step [5/9], Loss: 0.0062\n",
      "Epoch [70/200], Step [6/9], Loss: 0.0061\n",
      "Epoch [70/200], Step [7/9], Loss: 0.0075\n",
      "Epoch [70/200], Step [8/9], Loss: 0.0069\n",
      "Epoch [71/200], Step [1/9], Loss: 0.0076\n",
      "Epoch [71/200], Step [2/9], Loss: 0.0061\n",
      "Epoch [71/200], Step [3/9], Loss: 0.0060\n",
      "Epoch [71/200], Step [4/9], Loss: 0.0079\n",
      "Epoch [71/200], Step [5/9], Loss: 0.0055\n",
      "Epoch [71/200], Step [6/9], Loss: 0.0073\n",
      "Epoch [71/200], Step [7/9], Loss: 0.0065\n",
      "Epoch [71/200], Step [8/9], Loss: 0.0060\n",
      "Epoch [72/200], Step [1/9], Loss: 0.0059\n",
      "Epoch [72/200], Step [2/9], Loss: 0.0072\n",
      "Epoch [72/200], Step [3/9], Loss: 0.0067\n",
      "Epoch [72/200], Step [4/9], Loss: 0.0051\n",
      "Epoch [72/200], Step [5/9], Loss: 0.0062\n",
      "Epoch [72/200], Step [6/9], Loss: 0.0069\n",
      "Epoch [72/200], Step [7/9], Loss: 0.0063\n",
      "Epoch [72/200], Step [8/9], Loss: 0.0084\n",
      "Epoch [73/200], Step [1/9], Loss: 0.0063\n",
      "Epoch [73/200], Step [2/9], Loss: 0.0075\n",
      "Epoch [73/200], Step [3/9], Loss: 0.0069\n",
      "Epoch [73/200], Step [4/9], Loss: 0.0068\n",
      "Epoch [73/200], Step [5/9], Loss: 0.0049\n",
      "Epoch [73/200], Step [6/9], Loss: 0.0072\n",
      "Epoch [73/200], Step [7/9], Loss: 0.0050\n",
      "Epoch [73/200], Step [8/9], Loss: 0.0071\n",
      "Epoch [74/200], Step [1/9], Loss: 0.0065\n",
      "Epoch [74/200], Step [2/9], Loss: 0.0053\n",
      "Epoch [74/200], Step [3/9], Loss: 0.0057\n",
      "Epoch [74/200], Step [4/9], Loss: 0.0051\n",
      "Epoch [74/200], Step [5/9], Loss: 0.0066\n",
      "Epoch [74/200], Step [6/9], Loss: 0.0074\n",
      "Epoch [74/200], Step [7/9], Loss: 0.0050\n",
      "Epoch [74/200], Step [8/9], Loss: 0.0060\n",
      "Epoch [75/200], Step [1/9], Loss: 0.0072\n",
      "Epoch [75/200], Step [2/9], Loss: 0.0063\n",
      "Epoch [75/200], Step [3/9], Loss: 0.0080\n",
      "Epoch [75/200], Step [4/9], Loss: 0.0069\n",
      "Epoch [75/200], Step [5/9], Loss: 0.0058\n",
      "Epoch [75/200], Step [6/9], Loss: 0.0051\n",
      "Epoch [75/200], Step [7/9], Loss: 0.0074\n",
      "Epoch [75/200], Step [8/9], Loss: 0.0069\n",
      "Epoch [76/200], Step [1/9], Loss: 0.0059\n",
      "Epoch [76/200], Step [2/9], Loss: 0.0068\n",
      "Epoch [76/200], Step [3/9], Loss: 0.0047\n",
      "Epoch [76/200], Step [4/9], Loss: 0.0062\n",
      "Epoch [76/200], Step [5/9], Loss: 0.0071\n",
      "Epoch [76/200], Step [6/9], Loss: 0.0050\n",
      "Epoch [76/200], Step [7/9], Loss: 0.0073\n",
      "Epoch [76/200], Step [8/9], Loss: 0.0063\n",
      "Epoch [77/200], Step [1/9], Loss: 0.0072\n",
      "Epoch [77/200], Step [2/9], Loss: 0.0057\n",
      "Epoch [77/200], Step [3/9], Loss: 0.0064\n",
      "Epoch [77/200], Step [4/9], Loss: 0.0059\n",
      "Epoch [77/200], Step [5/9], Loss: 0.0066\n",
      "Epoch [77/200], Step [6/9], Loss: 0.0057\n",
      "Epoch [77/200], Step [7/9], Loss: 0.0048\n",
      "Epoch [77/200], Step [8/9], Loss: 0.0066\n",
      "Epoch [78/200], Step [1/9], Loss: 0.0063\n",
      "Epoch [78/200], Step [2/9], Loss: 0.0060\n",
      "Epoch [78/200], Step [3/9], Loss: 0.0071\n",
      "Epoch [78/200], Step [4/9], Loss: 0.0061\n",
      "Epoch [78/200], Step [5/9], Loss: 0.0057\n",
      "Epoch [78/200], Step [6/9], Loss: 0.0046\n",
      "Epoch [78/200], Step [7/9], Loss: 0.0070\n",
      "Epoch [78/200], Step [8/9], Loss: 0.0060\n",
      "Epoch [79/200], Step [1/9], Loss: 0.0057\n",
      "Epoch [79/200], Step [2/9], Loss: 0.0049\n",
      "Epoch [79/200], Step [3/9], Loss: 0.0062\n",
      "Epoch [79/200], Step [4/9], Loss: 0.0068\n",
      "Epoch [79/200], Step [5/9], Loss: 0.0090\n",
      "Epoch [79/200], Step [6/9], Loss: 0.0059\n",
      "Epoch [79/200], Step [7/9], Loss: 0.0055\n",
      "Epoch [79/200], Step [8/9], Loss: 0.0065\n",
      "Epoch [80/200], Step [1/9], Loss: 0.0065\n",
      "Epoch [80/200], Step [2/9], Loss: 0.0073\n",
      "Epoch [80/200], Step [3/9], Loss: 0.0072\n",
      "Epoch [80/200], Step [4/9], Loss: 0.0076\n",
      "Epoch [80/200], Step [5/9], Loss: 0.0072\n",
      "Epoch [80/200], Step [6/9], Loss: 0.0069\n",
      "Epoch [80/200], Step [7/9], Loss: 0.0055\n",
      "Epoch [80/200], Step [8/9], Loss: 0.0070\n",
      "Epoch [81/200], Step [1/9], Loss: 0.0073\n",
      "Epoch [81/200], Step [2/9], Loss: 0.0049\n",
      "Epoch [81/200], Step [3/9], Loss: 0.0055\n",
      "Epoch [81/200], Step [4/9], Loss: 0.0050\n",
      "Epoch [81/200], Step [5/9], Loss: 0.0079\n",
      "Epoch [81/200], Step [6/9], Loss: 0.0059\n",
      "Epoch [81/200], Step [7/9], Loss: 0.0061\n",
      "Epoch [81/200], Step [8/9], Loss: 0.0073\n",
      "Epoch [82/200], Step [1/9], Loss: 0.0054\n",
      "Epoch [82/200], Step [2/9], Loss: 0.0053\n",
      "Epoch [82/200], Step [3/9], Loss: 0.0073\n",
      "Epoch [82/200], Step [4/9], Loss: 0.0068\n",
      "Epoch [82/200], Step [5/9], Loss: 0.0071\n",
      "Epoch [82/200], Step [6/9], Loss: 0.0066\n",
      "Epoch [82/200], Step [7/9], Loss: 0.0050\n",
      "Epoch [82/200], Step [8/9], Loss: 0.0059\n",
      "Epoch [83/200], Step [1/9], Loss: 0.0073\n",
      "Epoch [83/200], Step [2/9], Loss: 0.0049\n",
      "Epoch [83/200], Step [3/9], Loss: 0.0074\n",
      "Epoch [83/200], Step [4/9], Loss: 0.0066\n",
      "Epoch [83/200], Step [5/9], Loss: 0.0060\n",
      "Epoch [83/200], Step [6/9], Loss: 0.0081\n",
      "Epoch [83/200], Step [7/9], Loss: 0.0068\n",
      "Epoch [83/200], Step [8/9], Loss: 0.0072\n",
      "Epoch [84/200], Step [1/9], Loss: 0.0061\n",
      "Epoch [84/200], Step [2/9], Loss: 0.0075\n",
      "Epoch [84/200], Step [3/9], Loss: 0.0049\n",
      "Epoch [84/200], Step [4/9], Loss: 0.0058\n",
      "Epoch [84/200], Step [5/9], Loss: 0.0058\n",
      "Epoch [84/200], Step [6/9], Loss: 0.0061\n",
      "Epoch [84/200], Step [7/9], Loss: 0.0077\n",
      "Epoch [84/200], Step [8/9], Loss: 0.0080\n",
      "Epoch [85/200], Step [1/9], Loss: 0.0057\n",
      "Epoch [85/200], Step [2/9], Loss: 0.0067\n",
      "Epoch [85/200], Step [3/9], Loss: 0.0074\n",
      "Epoch [85/200], Step [4/9], Loss: 0.0067\n",
      "Epoch [85/200], Step [5/9], Loss: 0.0053\n",
      "Epoch [85/200], Step [6/9], Loss: 0.0068\n",
      "Epoch [85/200], Step [7/9], Loss: 0.0060\n",
      "Epoch [85/200], Step [8/9], Loss: 0.0054\n",
      "Epoch [86/200], Step [1/9], Loss: 0.0071\n",
      "Epoch [86/200], Step [2/9], Loss: 0.0066\n",
      "Epoch [86/200], Step [3/9], Loss: 0.0070\n",
      "Epoch [86/200], Step [4/9], Loss: 0.0067\n",
      "Epoch [86/200], Step [5/9], Loss: 0.0072\n",
      "Epoch [86/200], Step [6/9], Loss: 0.0061\n",
      "Epoch [86/200], Step [7/9], Loss: 0.0061\n",
      "Epoch [86/200], Step [8/9], Loss: 0.0061\n",
      "Epoch [87/200], Step [1/9], Loss: 0.0079\n",
      "Epoch [87/200], Step [2/9], Loss: 0.0068\n",
      "Epoch [87/200], Step [3/9], Loss: 0.0060\n",
      "Epoch [87/200], Step [4/9], Loss: 0.0060\n",
      "Epoch [87/200], Step [5/9], Loss: 0.0059\n",
      "Epoch [87/200], Step [6/9], Loss: 0.0061\n",
      "Epoch [87/200], Step [7/9], Loss: 0.0066\n",
      "Epoch [87/200], Step [8/9], Loss: 0.0066\n",
      "Epoch [88/200], Step [1/9], Loss: 0.0081\n",
      "Epoch [88/200], Step [2/9], Loss: 0.0061\n",
      "Epoch [88/200], Step [3/9], Loss: 0.0068\n",
      "Epoch [88/200], Step [4/9], Loss: 0.0058\n",
      "Epoch [88/200], Step [5/9], Loss: 0.0079\n",
      "Epoch [88/200], Step [6/9], Loss: 0.0058\n",
      "Epoch [88/200], Step [7/9], Loss: 0.0052\n",
      "Epoch [88/200], Step [8/9], Loss: 0.0066\n",
      "Epoch [89/200], Step [1/9], Loss: 0.0063\n",
      "Epoch [89/200], Step [2/9], Loss: 0.0052\n",
      "Epoch [89/200], Step [3/9], Loss: 0.0094\n",
      "Epoch [89/200], Step [4/9], Loss: 0.0089\n",
      "Epoch [89/200], Step [5/9], Loss: 0.0071\n",
      "Epoch [89/200], Step [6/9], Loss: 0.0069\n",
      "Epoch [89/200], Step [7/9], Loss: 0.0072\n",
      "Epoch [89/200], Step [8/9], Loss: 0.0079\n",
      "Epoch [90/200], Step [1/9], Loss: 0.0068\n",
      "Epoch [90/200], Step [2/9], Loss: 0.0056\n",
      "Epoch [90/200], Step [3/9], Loss: 0.0061\n",
      "Epoch [90/200], Step [4/9], Loss: 0.0062\n",
      "Epoch [90/200], Step [5/9], Loss: 0.0069\n",
      "Epoch [90/200], Step [6/9], Loss: 0.0074\n",
      "Epoch [90/200], Step [7/9], Loss: 0.0055\n",
      "Epoch [90/200], Step [8/9], Loss: 0.0074\n",
      "Epoch [91/200], Step [1/9], Loss: 0.0071\n",
      "Epoch [91/200], Step [2/9], Loss: 0.0060\n",
      "Epoch [91/200], Step [3/9], Loss: 0.0071\n",
      "Epoch [91/200], Step [4/9], Loss: 0.0059\n",
      "Epoch [91/200], Step [5/9], Loss: 0.0061\n",
      "Epoch [91/200], Step [6/9], Loss: 0.0075\n",
      "Epoch [91/200], Step [7/9], Loss: 0.0068\n",
      "Epoch [91/200], Step [8/9], Loss: 0.0066\n",
      "Epoch [92/200], Step [1/9], Loss: 0.0063\n",
      "Epoch [92/200], Step [2/9], Loss: 0.0084\n",
      "Epoch [92/200], Step [3/9], Loss: 0.0053\n",
      "Epoch [92/200], Step [4/9], Loss: 0.0066\n",
      "Epoch [92/200], Step [5/9], Loss: 0.0053\n",
      "Epoch [92/200], Step [6/9], Loss: 0.0072\n",
      "Epoch [92/200], Step [7/9], Loss: 0.0067\n",
      "Epoch [92/200], Step [8/9], Loss: 0.0071\n",
      "Epoch [93/200], Step [1/9], Loss: 0.0074\n",
      "Epoch [93/200], Step [2/9], Loss: 0.0063\n",
      "Epoch [93/200], Step [3/9], Loss: 0.0059\n",
      "Epoch [93/200], Step [4/9], Loss: 0.0086\n",
      "Epoch [93/200], Step [5/9], Loss: 0.0077\n",
      "Epoch [93/200], Step [6/9], Loss: 0.0057\n",
      "Epoch [93/200], Step [7/9], Loss: 0.0075\n",
      "Epoch [93/200], Step [8/9], Loss: 0.0059\n",
      "Epoch [94/200], Step [1/9], Loss: 0.0060\n",
      "Epoch [94/200], Step [2/9], Loss: 0.0053\n",
      "Epoch [94/200], Step [3/9], Loss: 0.0070\n",
      "Epoch [94/200], Step [4/9], Loss: 0.0057\n",
      "Epoch [94/200], Step [5/9], Loss: 0.0056\n",
      "Epoch [94/200], Step [6/9], Loss: 0.0068\n",
      "Epoch [94/200], Step [7/9], Loss: 0.0092\n",
      "Epoch [94/200], Step [8/9], Loss: 0.0070\n",
      "Epoch [95/200], Step [1/9], Loss: 0.0077\n",
      "Epoch [95/200], Step [2/9], Loss: 0.0056\n",
      "Epoch [95/200], Step [3/9], Loss: 0.0058\n",
      "Epoch [95/200], Step [4/9], Loss: 0.0071\n",
      "Epoch [95/200], Step [5/9], Loss: 0.0052\n",
      "Epoch [95/200], Step [6/9], Loss: 0.0075\n",
      "Epoch [95/200], Step [7/9], Loss: 0.0063\n",
      "Epoch [95/200], Step [8/9], Loss: 0.0080\n",
      "Epoch [96/200], Step [1/9], Loss: 0.0075\n",
      "Epoch [96/200], Step [2/9], Loss: 0.0076\n",
      "Epoch [96/200], Step [3/9], Loss: 0.0078\n",
      "Epoch [96/200], Step [4/9], Loss: 0.0079\n",
      "Epoch [96/200], Step [5/9], Loss: 0.0066\n",
      "Epoch [96/200], Step [6/9], Loss: 0.0074\n",
      "Epoch [96/200], Step [7/9], Loss: 0.0058\n",
      "Epoch [96/200], Step [8/9], Loss: 0.0064\n",
      "Epoch [97/200], Step [1/9], Loss: 0.0054\n",
      "Epoch [97/200], Step [2/9], Loss: 0.0080\n",
      "Epoch [97/200], Step [3/9], Loss: 0.0057\n",
      "Epoch [97/200], Step [4/9], Loss: 0.0070\n",
      "Epoch [97/200], Step [5/9], Loss: 0.0075\n",
      "Epoch [97/200], Step [6/9], Loss: 0.0073\n",
      "Epoch [97/200], Step [7/9], Loss: 0.0059\n",
      "Epoch [97/200], Step [8/9], Loss: 0.0069\n",
      "Epoch [98/200], Step [1/9], Loss: 0.0067\n",
      "Epoch [98/200], Step [2/9], Loss: 0.0071\n",
      "Epoch [98/200], Step [3/9], Loss: 0.0055\n",
      "Epoch [98/200], Step [4/9], Loss: 0.0068\n",
      "Epoch [98/200], Step [5/9], Loss: 0.0077\n",
      "Epoch [98/200], Step [6/9], Loss: 0.0070\n",
      "Epoch [98/200], Step [7/9], Loss: 0.0080\n",
      "Epoch [98/200], Step [8/9], Loss: 0.0073\n",
      "Epoch [99/200], Step [1/9], Loss: 0.0071\n",
      "Epoch [99/200], Step [2/9], Loss: 0.0058\n",
      "Epoch [99/200], Step [3/9], Loss: 0.0059\n",
      "Epoch [99/200], Step [4/9], Loss: 0.0067\n",
      "Epoch [99/200], Step [5/9], Loss: 0.0064\n",
      "Epoch [99/200], Step [6/9], Loss: 0.0070\n",
      "Epoch [99/200], Step [7/9], Loss: 0.0051\n",
      "Epoch [99/200], Step [8/9], Loss: 0.0071\n",
      "Epoch [100/200], Step [1/9], Loss: 0.0051\n",
      "Epoch [100/200], Step [2/9], Loss: 0.0062\n",
      "Epoch [100/200], Step [3/9], Loss: 0.0070\n",
      "Epoch [100/200], Step [4/9], Loss: 0.0057\n",
      "Epoch [100/200], Step [5/9], Loss: 0.0070\n",
      "Epoch [100/200], Step [6/9], Loss: 0.0070\n",
      "Epoch [100/200], Step [7/9], Loss: 0.0064\n",
      "Epoch [100/200], Step [8/9], Loss: 0.0070\n",
      "Epoch [101/200], Step [1/9], Loss: 0.0049\n",
      "Epoch [101/200], Step [2/9], Loss: 0.0063\n",
      "Epoch [101/200], Step [3/9], Loss: 0.0071\n",
      "Epoch [101/200], Step [4/9], Loss: 0.0060\n",
      "Epoch [101/200], Step [5/9], Loss: 0.0086\n",
      "Epoch [101/200], Step [6/9], Loss: 0.0057\n",
      "Epoch [101/200], Step [7/9], Loss: 0.0052\n",
      "Epoch [101/200], Step [8/9], Loss: 0.0059\n",
      "Epoch [102/200], Step [1/9], Loss: 0.0059\n",
      "Epoch [102/200], Step [2/9], Loss: 0.0049\n",
      "Epoch [102/200], Step [3/9], Loss: 0.0070\n",
      "Epoch [102/200], Step [4/9], Loss: 0.0070\n",
      "Epoch [102/200], Step [5/9], Loss: 0.0065\n",
      "Epoch [102/200], Step [6/9], Loss: 0.0071\n",
      "Epoch [102/200], Step [7/9], Loss: 0.0062\n",
      "Epoch [102/200], Step [8/9], Loss: 0.0061\n",
      "Epoch [103/200], Step [1/9], Loss: 0.0054\n",
      "Epoch [103/200], Step [2/9], Loss: 0.0047\n",
      "Epoch [103/200], Step [3/9], Loss: 0.0054\n",
      "Epoch [103/200], Step [4/9], Loss: 0.0068\n",
      "Epoch [103/200], Step [5/9], Loss: 0.0070\n",
      "Epoch [103/200], Step [6/9], Loss: 0.0071\n",
      "Epoch [103/200], Step [7/9], Loss: 0.0060\n",
      "Epoch [103/200], Step [8/9], Loss: 0.0068\n",
      "Epoch [104/200], Step [1/9], Loss: 0.0050\n",
      "Epoch [104/200], Step [2/9], Loss: 0.0054\n",
      "Epoch [104/200], Step [3/9], Loss: 0.0054\n",
      "Epoch [104/200], Step [4/9], Loss: 0.0083\n",
      "Epoch [104/200], Step [5/9], Loss: 0.0058\n",
      "Epoch [104/200], Step [6/9], Loss: 0.0044\n",
      "Epoch [104/200], Step [7/9], Loss: 0.0064\n",
      "Epoch [104/200], Step [8/9], Loss: 0.0068\n",
      "Epoch [105/200], Step [1/9], Loss: 0.0059\n",
      "Epoch [105/200], Step [2/9], Loss: 0.0067\n",
      "Epoch [105/200], Step [3/9], Loss: 0.0063\n",
      "Epoch [105/200], Step [4/9], Loss: 0.0055\n",
      "Epoch [105/200], Step [5/9], Loss: 0.0049\n",
      "Epoch [105/200], Step [6/9], Loss: 0.0048\n",
      "Epoch [105/200], Step [7/9], Loss: 0.0062\n",
      "Epoch [105/200], Step [8/9], Loss: 0.0056\n",
      "Epoch [106/200], Step [1/9], Loss: 0.0062\n",
      "Epoch [106/200], Step [2/9], Loss: 0.0055\n",
      "Epoch [106/200], Step [3/9], Loss: 0.0073\n",
      "Epoch [106/200], Step [4/9], Loss: 0.0067\n",
      "Epoch [106/200], Step [5/9], Loss: 0.0060\n",
      "Epoch [106/200], Step [6/9], Loss: 0.0060\n",
      "Epoch [106/200], Step [7/9], Loss: 0.0068\n",
      "Epoch [106/200], Step [8/9], Loss: 0.0062\n",
      "Epoch [107/200], Step [1/9], Loss: 0.0051\n",
      "Epoch [107/200], Step [2/9], Loss: 0.0054\n",
      "Epoch [107/200], Step [3/9], Loss: 0.0053\n",
      "Epoch [107/200], Step [4/9], Loss: 0.0058\n",
      "Epoch [107/200], Step [5/9], Loss: 0.0065\n",
      "Epoch [107/200], Step [6/9], Loss: 0.0064\n",
      "Epoch [107/200], Step [7/9], Loss: 0.0062\n",
      "Epoch [107/200], Step [8/9], Loss: 0.0069\n",
      "Epoch [108/200], Step [1/9], Loss: 0.0061\n",
      "Epoch [108/200], Step [2/9], Loss: 0.0048\n",
      "Epoch [108/200], Step [3/9], Loss: 0.0066\n",
      "Epoch [108/200], Step [4/9], Loss: 0.0064\n",
      "Epoch [108/200], Step [5/9], Loss: 0.0066\n",
      "Epoch [108/200], Step [6/9], Loss: 0.0060\n",
      "Epoch [108/200], Step [7/9], Loss: 0.0068\n",
      "Epoch [108/200], Step [8/9], Loss: 0.0063\n",
      "Epoch [109/200], Step [1/9], Loss: 0.0046\n",
      "Epoch [109/200], Step [2/9], Loss: 0.0045\n",
      "Epoch [109/200], Step [3/9], Loss: 0.0053\n",
      "Epoch [109/200], Step [4/9], Loss: 0.0052\n",
      "Epoch [109/200], Step [5/9], Loss: 0.0050\n",
      "Epoch [109/200], Step [6/9], Loss: 0.0054\n",
      "Epoch [109/200], Step [7/9], Loss: 0.0060\n",
      "Epoch [109/200], Step [8/9], Loss: 0.0049\n",
      "Epoch [110/200], Step [1/9], Loss: 0.0046\n",
      "Epoch [110/200], Step [2/9], Loss: 0.0074\n",
      "Epoch [110/200], Step [3/9], Loss: 0.0053\n",
      "Epoch [110/200], Step [4/9], Loss: 0.0062\n",
      "Epoch [110/200], Step [5/9], Loss: 0.0050\n",
      "Epoch [110/200], Step [6/9], Loss: 0.0054\n",
      "Epoch [110/200], Step [7/9], Loss: 0.0057\n",
      "Epoch [110/200], Step [8/9], Loss: 0.0064\n",
      "Epoch [111/200], Step [1/9], Loss: 0.0064\n",
      "Epoch [111/200], Step [2/9], Loss: 0.0053\n",
      "Epoch [111/200], Step [3/9], Loss: 0.0076\n",
      "Epoch [111/200], Step [4/9], Loss: 0.0052\n",
      "Epoch [111/200], Step [5/9], Loss: 0.0043\n",
      "Epoch [111/200], Step [6/9], Loss: 0.0050\n",
      "Epoch [111/200], Step [7/9], Loss: 0.0049\n",
      "Epoch [111/200], Step [8/9], Loss: 0.0068\n",
      "Epoch [112/200], Step [1/9], Loss: 0.0062\n",
      "Epoch [112/200], Step [2/9], Loss: 0.0064\n",
      "Epoch [112/200], Step [3/9], Loss: 0.0072\n",
      "Epoch [112/200], Step [4/9], Loss: 0.0062\n",
      "Epoch [112/200], Step [5/9], Loss: 0.0067\n",
      "Epoch [112/200], Step [6/9], Loss: 0.0059\n",
      "Epoch [112/200], Step [7/9], Loss: 0.0052\n",
      "Epoch [112/200], Step [8/9], Loss: 0.0068\n",
      "Epoch [113/200], Step [1/9], Loss: 0.0077\n",
      "Epoch [113/200], Step [2/9], Loss: 0.0075\n",
      "Epoch [113/200], Step [3/9], Loss: 0.0063\n",
      "Epoch [113/200], Step [4/9], Loss: 0.0060\n",
      "Epoch [113/200], Step [5/9], Loss: 0.0062\n",
      "Epoch [113/200], Step [6/9], Loss: 0.0063\n",
      "Epoch [113/200], Step [7/9], Loss: 0.0059\n",
      "Epoch [113/200], Step [8/9], Loss: 0.0067\n",
      "Epoch [114/200], Step [1/9], Loss: 0.0058\n",
      "Epoch [114/200], Step [2/9], Loss: 0.0048\n",
      "Epoch [114/200], Step [3/9], Loss: 0.0060\n",
      "Epoch [114/200], Step [4/9], Loss: 0.0068\n",
      "Epoch [114/200], Step [5/9], Loss: 0.0054\n",
      "Epoch [114/200], Step [6/9], Loss: 0.0062\n",
      "Epoch [114/200], Step [7/9], Loss: 0.0052\n",
      "Epoch [114/200], Step [8/9], Loss: 0.0057\n",
      "Epoch [115/200], Step [1/9], Loss: 0.0052\n",
      "Epoch [115/200], Step [2/9], Loss: 0.0064\n",
      "Epoch [115/200], Step [3/9], Loss: 0.0062\n",
      "Epoch [115/200], Step [4/9], Loss: 0.0065\n",
      "Epoch [115/200], Step [5/9], Loss: 0.0063\n",
      "Epoch [115/200], Step [6/9], Loss: 0.0046\n",
      "Epoch [115/200], Step [7/9], Loss: 0.0056\n",
      "Epoch [115/200], Step [8/9], Loss: 0.0051\n",
      "Epoch [116/200], Step [1/9], Loss: 0.0066\n",
      "Epoch [116/200], Step [2/9], Loss: 0.0056\n",
      "Epoch [116/200], Step [3/9], Loss: 0.0069\n",
      "Epoch [116/200], Step [4/9], Loss: 0.0063\n",
      "Epoch [116/200], Step [5/9], Loss: 0.0075\n",
      "Epoch [116/200], Step [6/9], Loss: 0.0086\n",
      "Epoch [116/200], Step [7/9], Loss: 0.0058\n",
      "Epoch [116/200], Step [8/9], Loss: 0.0058\n",
      "Epoch [117/200], Step [1/9], Loss: 0.0057\n",
      "Epoch [117/200], Step [2/9], Loss: 0.0086\n",
      "Epoch [117/200], Step [3/9], Loss: 0.0051\n",
      "Epoch [117/200], Step [4/9], Loss: 0.0058\n",
      "Epoch [117/200], Step [5/9], Loss: 0.0061\n",
      "Epoch [117/200], Step [6/9], Loss: 0.0050\n",
      "Epoch [117/200], Step [7/9], Loss: 0.0053\n",
      "Epoch [117/200], Step [8/9], Loss: 0.0075\n",
      "Epoch [118/200], Step [1/9], Loss: 0.0045\n",
      "Epoch [118/200], Step [2/9], Loss: 0.0045\n",
      "Epoch [118/200], Step [3/9], Loss: 0.0049\n",
      "Epoch [118/200], Step [4/9], Loss: 0.0050\n",
      "Epoch [118/200], Step [5/9], Loss: 0.0055\n",
      "Epoch [118/200], Step [6/9], Loss: 0.0059\n",
      "Epoch [118/200], Step [7/9], Loss: 0.0048\n",
      "Epoch [118/200], Step [8/9], Loss: 0.0046\n",
      "Epoch [119/200], Step [1/9], Loss: 0.0051\n",
      "Epoch [119/200], Step [2/9], Loss: 0.0051\n",
      "Epoch [119/200], Step [3/9], Loss: 0.0068\n",
      "Epoch [119/200], Step [4/9], Loss: 0.0044\n",
      "Epoch [119/200], Step [5/9], Loss: 0.0049\n",
      "Epoch [119/200], Step [6/9], Loss: 0.0075\n",
      "Epoch [119/200], Step [7/9], Loss: 0.0059\n",
      "Epoch [119/200], Step [8/9], Loss: 0.0057\n",
      "Epoch [120/200], Step [1/9], Loss: 0.0044\n",
      "Epoch [120/200], Step [2/9], Loss: 0.0051\n",
      "Epoch [120/200], Step [3/9], Loss: 0.0071\n",
      "Epoch [120/200], Step [4/9], Loss: 0.0057\n",
      "Epoch [120/200], Step [5/9], Loss: 0.0057\n",
      "Epoch [120/200], Step [6/9], Loss: 0.0069\n",
      "Epoch [120/200], Step [7/9], Loss: 0.0074\n",
      "Epoch [120/200], Step [8/9], Loss: 0.0046\n",
      "Epoch [121/200], Step [1/9], Loss: 0.0042\n",
      "Epoch [121/200], Step [2/9], Loss: 0.0052\n",
      "Epoch [121/200], Step [3/9], Loss: 0.0050\n",
      "Epoch [121/200], Step [4/9], Loss: 0.0051\n",
      "Epoch [121/200], Step [5/9], Loss: 0.0059\n",
      "Epoch [121/200], Step [6/9], Loss: 0.0069\n",
      "Epoch [121/200], Step [7/9], Loss: 0.0055\n",
      "Epoch [121/200], Step [8/9], Loss: 0.0053\n",
      "Epoch [122/200], Step [1/9], Loss: 0.0063\n",
      "Epoch [122/200], Step [2/9], Loss: 0.0069\n",
      "Epoch [122/200], Step [3/9], Loss: 0.0062\n",
      "Epoch [122/200], Step [4/9], Loss: 0.0049\n",
      "Epoch [122/200], Step [5/9], Loss: 0.0051\n",
      "Epoch [122/200], Step [6/9], Loss: 0.0064\n",
      "Epoch [122/200], Step [7/9], Loss: 0.0060\n",
      "Epoch [122/200], Step [8/9], Loss: 0.0041\n",
      "Epoch [123/200], Step [1/9], Loss: 0.0042\n",
      "Epoch [123/200], Step [2/9], Loss: 0.0113\n",
      "Epoch [123/200], Step [3/9], Loss: 0.0066\n",
      "Epoch [123/200], Step [4/9], Loss: 0.0066\n",
      "Epoch [123/200], Step [5/9], Loss: 0.0048\n",
      "Epoch [123/200], Step [6/9], Loss: 0.0062\n",
      "Epoch [123/200], Step [7/9], Loss: 0.0058\n",
      "Epoch [123/200], Step [8/9], Loss: 0.0048\n",
      "Epoch [124/200], Step [1/9], Loss: 0.0046\n",
      "Epoch [124/200], Step [2/9], Loss: 0.0053\n",
      "Epoch [124/200], Step [3/9], Loss: 0.0077\n",
      "Epoch [124/200], Step [4/9], Loss: 0.0056\n",
      "Epoch [124/200], Step [5/9], Loss: 0.0053\n",
      "Epoch [124/200], Step [6/9], Loss: 0.0059\n",
      "Epoch [124/200], Step [7/9], Loss: 0.0053\n",
      "Epoch [124/200], Step [8/9], Loss: 0.0074\n",
      "Epoch [125/200], Step [1/9], Loss: 0.0049\n",
      "Epoch [125/200], Step [2/9], Loss: 0.0047\n",
      "Epoch [125/200], Step [3/9], Loss: 0.0065\n",
      "Epoch [125/200], Step [4/9], Loss: 0.0046\n",
      "Epoch [125/200], Step [5/9], Loss: 0.0054\n",
      "Epoch [125/200], Step [6/9], Loss: 0.0066\n",
      "Epoch [125/200], Step [7/9], Loss: 0.0057\n",
      "Epoch [125/200], Step [8/9], Loss: 0.0042\n",
      "Epoch [126/200], Step [1/9], Loss: 0.0051\n",
      "Epoch [126/200], Step [2/9], Loss: 0.0055\n",
      "Epoch [126/200], Step [3/9], Loss: 0.0052\n",
      "Epoch [126/200], Step [4/9], Loss: 0.0054\n",
      "Epoch [126/200], Step [5/9], Loss: 0.0042\n",
      "Epoch [126/200], Step [6/9], Loss: 0.0046\n",
      "Epoch [126/200], Step [7/9], Loss: 0.0041\n",
      "Epoch [126/200], Step [8/9], Loss: 0.0044\n",
      "Epoch [127/200], Step [1/9], Loss: 0.0043\n",
      "Epoch [127/200], Step [2/9], Loss: 0.0060\n",
      "Epoch [127/200], Step [3/9], Loss: 0.0060\n",
      "Epoch [127/200], Step [4/9], Loss: 0.0051\n",
      "Epoch [127/200], Step [5/9], Loss: 0.0041\n",
      "Epoch [127/200], Step [6/9], Loss: 0.0066\n",
      "Epoch [127/200], Step [7/9], Loss: 0.0063\n",
      "Epoch [127/200], Step [8/9], Loss: 0.0054\n",
      "Epoch [128/200], Step [1/9], Loss: 0.0044\n",
      "Epoch [128/200], Step [2/9], Loss: 0.0069\n",
      "Epoch [128/200], Step [3/9], Loss: 0.0064\n",
      "Epoch [128/200], Step [4/9], Loss: 0.0047\n",
      "Epoch [128/200], Step [5/9], Loss: 0.0076\n",
      "Epoch [128/200], Step [6/9], Loss: 0.0068\n",
      "Epoch [128/200], Step [7/9], Loss: 0.0070\n",
      "Epoch [128/200], Step [8/9], Loss: 0.0048\n",
      "Epoch [129/200], Step [1/9], Loss: 0.0065\n",
      "Epoch [129/200], Step [2/9], Loss: 0.0061\n",
      "Epoch [129/200], Step [3/9], Loss: 0.0054\n",
      "Epoch [129/200], Step [4/9], Loss: 0.0054\n",
      "Epoch [129/200], Step [5/9], Loss: 0.0067\n",
      "Epoch [129/200], Step [6/9], Loss: 0.0061\n",
      "Epoch [129/200], Step [7/9], Loss: 0.0046\n",
      "Epoch [129/200], Step [8/9], Loss: 0.0049\n",
      "Epoch [130/200], Step [1/9], Loss: 0.0043\n",
      "Epoch [130/200], Step [2/9], Loss: 0.0053\n",
      "Epoch [130/200], Step [3/9], Loss: 0.0054\n",
      "Epoch [130/200], Step [4/9], Loss: 0.0061\n",
      "Epoch [130/200], Step [5/9], Loss: 0.0039\n",
      "Epoch [130/200], Step [6/9], Loss: 0.0055\n",
      "Epoch [130/200], Step [7/9], Loss: 0.0062\n",
      "Epoch [130/200], Step [8/9], Loss: 0.0047\n",
      "Epoch [131/200], Step [1/9], Loss: 0.0073\n",
      "Epoch [131/200], Step [2/9], Loss: 0.0055\n",
      "Epoch [131/200], Step [3/9], Loss: 0.0053\n",
      "Epoch [131/200], Step [4/9], Loss: 0.0083\n",
      "Epoch [131/200], Step [5/9], Loss: 0.0048\n",
      "Epoch [131/200], Step [6/9], Loss: 0.0069\n",
      "Epoch [131/200], Step [7/9], Loss: 0.0041\n",
      "Epoch [131/200], Step [8/9], Loss: 0.0068\n",
      "Epoch [132/200], Step [1/9], Loss: 0.0072\n",
      "Epoch [132/200], Step [2/9], Loss: 0.0042\n",
      "Epoch [132/200], Step [3/9], Loss: 0.0061\n",
      "Epoch [132/200], Step [4/9], Loss: 0.0045\n",
      "Epoch [132/200], Step [5/9], Loss: 0.0059\n",
      "Epoch [132/200], Step [6/9], Loss: 0.0058\n",
      "Epoch [132/200], Step [7/9], Loss: 0.0048\n",
      "Epoch [132/200], Step [8/9], Loss: 0.0053\n",
      "Epoch [133/200], Step [1/9], Loss: 0.0047\n",
      "Epoch [133/200], Step [2/9], Loss: 0.0055\n",
      "Epoch [133/200], Step [3/9], Loss: 0.0063\n",
      "Epoch [133/200], Step [4/9], Loss: 0.0064\n",
      "Epoch [133/200], Step [5/9], Loss: 0.0059\n",
      "Epoch [133/200], Step [6/9], Loss: 0.0073\n",
      "Epoch [133/200], Step [7/9], Loss: 0.0043\n",
      "Epoch [133/200], Step [8/9], Loss: 0.0073\n",
      "Epoch [134/200], Step [1/9], Loss: 0.0056\n",
      "Epoch [134/200], Step [2/9], Loss: 0.0067\n",
      "Epoch [134/200], Step [3/9], Loss: 0.0069\n",
      "Epoch [134/200], Step [4/9], Loss: 0.0055\n",
      "Epoch [134/200], Step [5/9], Loss: 0.0042\n",
      "Epoch [134/200], Step [6/9], Loss: 0.0060\n",
      "Epoch [134/200], Step [7/9], Loss: 0.0056\n",
      "Epoch [134/200], Step [8/9], Loss: 0.0045\n",
      "Epoch [135/200], Step [1/9], Loss: 0.0044\n",
      "Epoch [135/200], Step [2/9], Loss: 0.0064\n",
      "Epoch [135/200], Step [3/9], Loss: 0.0055\n",
      "Epoch [135/200], Step [4/9], Loss: 0.0046\n",
      "Epoch [135/200], Step [5/9], Loss: 0.0061\n",
      "Epoch [135/200], Step [6/9], Loss: 0.0068\n",
      "Epoch [135/200], Step [7/9], Loss: 0.0057\n",
      "Epoch [135/200], Step [8/9], Loss: 0.0051\n",
      "Epoch [136/200], Step [1/9], Loss: 0.0069\n",
      "Epoch [136/200], Step [2/9], Loss: 0.0050\n",
      "Epoch [136/200], Step [3/9], Loss: 0.0048\n",
      "Epoch [136/200], Step [4/9], Loss: 0.0066\n",
      "Epoch [136/200], Step [5/9], Loss: 0.0054\n",
      "Epoch [136/200], Step [6/9], Loss: 0.0046\n",
      "Epoch [136/200], Step [7/9], Loss: 0.0050\n",
      "Epoch [136/200], Step [8/9], Loss: 0.0060\n",
      "Epoch [137/200], Step [1/9], Loss: 0.0050\n",
      "Epoch [137/200], Step [2/9], Loss: 0.0043\n",
      "Epoch [137/200], Step [3/9], Loss: 0.0055\n",
      "Epoch [137/200], Step [4/9], Loss: 0.0050\n",
      "Epoch [137/200], Step [5/9], Loss: 0.0043\n",
      "Epoch [137/200], Step [6/9], Loss: 0.0065\n",
      "Epoch [137/200], Step [7/9], Loss: 0.0053\n",
      "Epoch [137/200], Step [8/9], Loss: 0.0062\n",
      "Epoch [138/200], Step [1/9], Loss: 0.0057\n",
      "Epoch [138/200], Step [2/9], Loss: 0.0083\n",
      "Epoch [138/200], Step [3/9], Loss: 0.0050\n",
      "Epoch [138/200], Step [4/9], Loss: 0.0065\n",
      "Epoch [138/200], Step [5/9], Loss: 0.0050\n",
      "Epoch [138/200], Step [6/9], Loss: 0.0066\n",
      "Epoch [138/200], Step [7/9], Loss: 0.0041\n",
      "Epoch [138/200], Step [8/9], Loss: 0.0067\n",
      "Epoch [139/200], Step [1/9], Loss: 0.0062\n",
      "Epoch [139/200], Step [2/9], Loss: 0.0047\n",
      "Epoch [139/200], Step [3/9], Loss: 0.0046\n",
      "Epoch [139/200], Step [4/9], Loss: 0.0046\n",
      "Epoch [139/200], Step [5/9], Loss: 0.0049\n",
      "Epoch [139/200], Step [6/9], Loss: 0.0063\n",
      "Epoch [139/200], Step [7/9], Loss: 0.0041\n",
      "Epoch [139/200], Step [8/9], Loss: 0.0056\n",
      "Epoch [140/200], Step [1/9], Loss: 0.0044\n",
      "Epoch [140/200], Step [2/9], Loss: 0.0053\n",
      "Epoch [140/200], Step [3/9], Loss: 0.0065\n",
      "Epoch [140/200], Step [4/9], Loss: 0.0065\n",
      "Epoch [140/200], Step [5/9], Loss: 0.0046\n",
      "Epoch [140/200], Step [6/9], Loss: 0.0047\n",
      "Epoch [140/200], Step [7/9], Loss: 0.0051\n",
      "Epoch [140/200], Step [8/9], Loss: 0.0049\n",
      "Epoch [141/200], Step [1/9], Loss: 0.0077\n",
      "Epoch [141/200], Step [2/9], Loss: 0.0068\n",
      "Epoch [141/200], Step [3/9], Loss: 0.0062\n",
      "Epoch [141/200], Step [4/9], Loss: 0.0062\n",
      "Epoch [141/200], Step [5/9], Loss: 0.0051\n",
      "Epoch [141/200], Step [6/9], Loss: 0.0046\n",
      "Epoch [141/200], Step [7/9], Loss: 0.0056\n",
      "Epoch [141/200], Step [8/9], Loss: 0.0044\n",
      "Epoch [142/200], Step [1/9], Loss: 0.0069\n",
      "Epoch [142/200], Step [2/9], Loss: 0.0044\n",
      "Epoch [142/200], Step [3/9], Loss: 0.0053\n",
      "Epoch [142/200], Step [4/9], Loss: 0.0052\n",
      "Epoch [142/200], Step [5/9], Loss: 0.0053\n",
      "Epoch [142/200], Step [6/9], Loss: 0.0046\n",
      "Epoch [142/200], Step [7/9], Loss: 0.0047\n",
      "Epoch [142/200], Step [8/9], Loss: 0.0069\n",
      "Epoch [143/200], Step [1/9], Loss: 0.0046\n",
      "Epoch [143/200], Step [2/9], Loss: 0.0041\n",
      "Epoch [143/200], Step [3/9], Loss: 0.0046\n",
      "Epoch [143/200], Step [4/9], Loss: 0.0053\n",
      "Epoch [143/200], Step [5/9], Loss: 0.0066\n",
      "Epoch [143/200], Step [6/9], Loss: 0.0079\n",
      "Epoch [143/200], Step [7/9], Loss: 0.0064\n",
      "Epoch [143/200], Step [8/9], Loss: 0.0052\n",
      "Epoch [144/200], Step [1/9], Loss: 0.0042\n",
      "Epoch [144/200], Step [2/9], Loss: 0.0067\n",
      "Epoch [144/200], Step [3/9], Loss: 0.0052\n",
      "Epoch [144/200], Step [4/9], Loss: 0.0056\n",
      "Epoch [144/200], Step [5/9], Loss: 0.0047\n",
      "Epoch [144/200], Step [6/9], Loss: 0.0047\n",
      "Epoch [144/200], Step [7/9], Loss: 0.0040\n",
      "Epoch [144/200], Step [8/9], Loss: 0.0046\n",
      "Epoch [145/200], Step [1/9], Loss: 0.0040\n",
      "Epoch [145/200], Step [2/9], Loss: 0.0048\n",
      "Epoch [145/200], Step [3/9], Loss: 0.0066\n",
      "Epoch [145/200], Step [4/9], Loss: 0.0068\n",
      "Epoch [145/200], Step [5/9], Loss: 0.0050\n",
      "Epoch [145/200], Step [6/9], Loss: 0.0051\n",
      "Epoch [145/200], Step [7/9], Loss: 0.0046\n",
      "Epoch [145/200], Step [8/9], Loss: 0.0045\n",
      "Epoch [146/200], Step [1/9], Loss: 0.0051\n",
      "Epoch [146/200], Step [2/9], Loss: 0.0065\n",
      "Epoch [146/200], Step [3/9], Loss: 0.0061\n",
      "Epoch [146/200], Step [4/9], Loss: 0.0048\n",
      "Epoch [146/200], Step [5/9], Loss: 0.0042\n",
      "Epoch [146/200], Step [6/9], Loss: 0.0055\n",
      "Epoch [146/200], Step [7/9], Loss: 0.0068\n",
      "Epoch [146/200], Step [8/9], Loss: 0.0049\n",
      "Epoch [147/200], Step [1/9], Loss: 0.0044\n",
      "Epoch [147/200], Step [2/9], Loss: 0.0066\n",
      "Epoch [147/200], Step [3/9], Loss: 0.0054\n",
      "Epoch [147/200], Step [4/9], Loss: 0.0060\n",
      "Epoch [147/200], Step [5/9], Loss: 0.0060\n",
      "Epoch [147/200], Step [6/9], Loss: 0.0044\n",
      "Epoch [147/200], Step [7/9], Loss: 0.0041\n",
      "Epoch [147/200], Step [8/9], Loss: 0.0059\n",
      "Epoch [148/200], Step [1/9], Loss: 0.0046\n",
      "Epoch [148/200], Step [2/9], Loss: 0.0058\n",
      "Epoch [148/200], Step [3/9], Loss: 0.0050\n",
      "Epoch [148/200], Step [4/9], Loss: 0.0082\n",
      "Epoch [148/200], Step [5/9], Loss: 0.0054\n",
      "Epoch [148/200], Step [6/9], Loss: 0.0049\n",
      "Epoch [148/200], Step [7/9], Loss: 0.0064\n",
      "Epoch [148/200], Step [8/9], Loss: 0.0043\n",
      "Epoch [149/200], Step [1/9], Loss: 0.0063\n",
      "Epoch [149/200], Step [2/9], Loss: 0.0048\n",
      "Epoch [149/200], Step [3/9], Loss: 0.0040\n",
      "Epoch [149/200], Step [4/9], Loss: 0.0061\n",
      "Epoch [149/200], Step [5/9], Loss: 0.0053\n",
      "Epoch [149/200], Step [6/9], Loss: 0.0067\n",
      "Epoch [149/200], Step [7/9], Loss: 0.0045\n",
      "Epoch [149/200], Step [8/9], Loss: 0.0053\n",
      "Epoch [150/200], Step [1/9], Loss: 0.0054\n",
      "Epoch [150/200], Step [2/9], Loss: 0.0052\n",
      "Epoch [150/200], Step [3/9], Loss: 0.0067\n",
      "Epoch [150/200], Step [4/9], Loss: 0.0044\n",
      "Epoch [150/200], Step [5/9], Loss: 0.0038\n",
      "Epoch [150/200], Step [6/9], Loss: 0.0046\n",
      "Epoch [150/200], Step [7/9], Loss: 0.0039\n",
      "Epoch [150/200], Step [8/9], Loss: 0.0064\n",
      "Epoch [151/200], Step [1/9], Loss: 0.0043\n",
      "Epoch [151/200], Step [2/9], Loss: 0.0045\n",
      "Epoch [151/200], Step [3/9], Loss: 0.0041\n",
      "Epoch [151/200], Step [4/9], Loss: 0.0037\n",
      "Epoch [151/200], Step [5/9], Loss: 0.0038\n",
      "Epoch [151/200], Step [6/9], Loss: 0.0044\n",
      "Epoch [151/200], Step [7/9], Loss: 0.0059\n",
      "Epoch [151/200], Step [8/9], Loss: 0.0063\n",
      "Epoch [152/200], Step [1/9], Loss: 0.0053\n",
      "Epoch [152/200], Step [2/9], Loss: 0.0064\n",
      "Epoch [152/200], Step [3/9], Loss: 0.0052\n",
      "Epoch [152/200], Step [4/9], Loss: 0.0039\n",
      "Epoch [152/200], Step [5/9], Loss: 0.0062\n",
      "Epoch [152/200], Step [6/9], Loss: 0.0060\n",
      "Epoch [152/200], Step [7/9], Loss: 0.0045\n",
      "Epoch [152/200], Step [8/9], Loss: 0.0059\n",
      "Epoch [153/200], Step [1/9], Loss: 0.0049\n",
      "Epoch [153/200], Step [2/9], Loss: 0.0060\n",
      "Epoch [153/200], Step [3/9], Loss: 0.0045\n",
      "Epoch [153/200], Step [4/9], Loss: 0.0037\n",
      "Epoch [153/200], Step [5/9], Loss: 0.0041\n",
      "Epoch [153/200], Step [6/9], Loss: 0.0065\n",
      "Epoch [153/200], Step [7/9], Loss: 0.0062\n",
      "Epoch [153/200], Step [8/9], Loss: 0.0047\n",
      "Epoch [154/200], Step [1/9], Loss: 0.0053\n",
      "Epoch [154/200], Step [2/9], Loss: 0.0059\n",
      "Epoch [154/200], Step [3/9], Loss: 0.0063\n",
      "Epoch [154/200], Step [4/9], Loss: 0.0041\n",
      "Epoch [154/200], Step [5/9], Loss: 0.0044\n",
      "Epoch [154/200], Step [6/9], Loss: 0.0037\n",
      "Epoch [154/200], Step [7/9], Loss: 0.0044\n",
      "Epoch [154/200], Step [8/9], Loss: 0.0053\n",
      "Epoch [155/200], Step [1/9], Loss: 0.0065\n",
      "Epoch [155/200], Step [2/9], Loss: 0.0040\n",
      "Epoch [155/200], Step [3/9], Loss: 0.0042\n",
      "Epoch [155/200], Step [4/9], Loss: 0.0061\n",
      "Epoch [155/200], Step [5/9], Loss: 0.0043\n",
      "Epoch [155/200], Step [6/9], Loss: 0.0043\n",
      "Epoch [155/200], Step [7/9], Loss: 0.0066\n",
      "Epoch [155/200], Step [8/9], Loss: 0.0049\n",
      "Epoch [156/200], Step [1/9], Loss: 0.0061\n",
      "Epoch [156/200], Step [2/9], Loss: 0.0044\n",
      "Epoch [156/200], Step [3/9], Loss: 0.0049\n",
      "Epoch [156/200], Step [4/9], Loss: 0.0044\n",
      "Epoch [156/200], Step [5/9], Loss: 0.0080\n",
      "Epoch [156/200], Step [6/9], Loss: 0.0062\n",
      "Epoch [156/200], Step [7/9], Loss: 0.0057\n",
      "Epoch [156/200], Step [8/9], Loss: 0.0060\n",
      "Epoch [157/200], Step [1/9], Loss: 0.0047\n",
      "Epoch [157/200], Step [2/9], Loss: 0.0056\n",
      "Epoch [157/200], Step [3/9], Loss: 0.0045\n",
      "Epoch [157/200], Step [4/9], Loss: 0.0042\n",
      "Epoch [157/200], Step [5/9], Loss: 0.0059\n",
      "Epoch [157/200], Step [6/9], Loss: 0.0041\n",
      "Epoch [157/200], Step [7/9], Loss: 0.0044\n",
      "Epoch [157/200], Step [8/9], Loss: 0.0062\n",
      "Epoch [158/200], Step [1/9], Loss: 0.0058\n",
      "Epoch [158/200], Step [2/9], Loss: 0.0071\n",
      "Epoch [158/200], Step [3/9], Loss: 0.0048\n",
      "Epoch [158/200], Step [4/9], Loss: 0.0077\n",
      "Epoch [158/200], Step [5/9], Loss: 0.0044\n",
      "Epoch [158/200], Step [6/9], Loss: 0.0046\n",
      "Epoch [158/200], Step [7/9], Loss: 0.0073\n",
      "Epoch [158/200], Step [8/9], Loss: 0.0052\n",
      "Epoch [159/200], Step [1/9], Loss: 0.0045\n",
      "Epoch [159/200], Step [2/9], Loss: 0.0046\n",
      "Epoch [159/200], Step [3/9], Loss: 0.0066\n",
      "Epoch [159/200], Step [4/9], Loss: 0.0042\n",
      "Epoch [159/200], Step [5/9], Loss: 0.0070\n",
      "Epoch [159/200], Step [6/9], Loss: 0.0053\n",
      "Epoch [159/200], Step [7/9], Loss: 0.0048\n",
      "Epoch [159/200], Step [8/9], Loss: 0.0038\n",
      "Epoch [160/200], Step [1/9], Loss: 0.0062\n",
      "Epoch [160/200], Step [2/9], Loss: 0.0062\n",
      "Epoch [160/200], Step [3/9], Loss: 0.0054\n",
      "Epoch [160/200], Step [4/9], Loss: 0.0045\n",
      "Epoch [160/200], Step [5/9], Loss: 0.0044\n",
      "Epoch [160/200], Step [6/9], Loss: 0.0041\n",
      "Epoch [160/200], Step [7/9], Loss: 0.0047\n",
      "Epoch [160/200], Step [8/9], Loss: 0.0051\n",
      "Epoch [161/200], Step [1/9], Loss: 0.0044\n",
      "Epoch [161/200], Step [2/9], Loss: 0.0050\n",
      "Epoch [161/200], Step [3/9], Loss: 0.0040\n",
      "Epoch [161/200], Step [4/9], Loss: 0.0072\n",
      "Epoch [161/200], Step [5/9], Loss: 0.0075\n",
      "Epoch [161/200], Step [6/9], Loss: 0.0049\n",
      "Epoch [161/200], Step [7/9], Loss: 0.0050\n",
      "Epoch [161/200], Step [8/9], Loss: 0.0042\n",
      "Epoch [162/200], Step [1/9], Loss: 0.0055\n",
      "Epoch [162/200], Step [2/9], Loss: 0.0040\n",
      "Epoch [162/200], Step [3/9], Loss: 0.0045\n",
      "Epoch [162/200], Step [4/9], Loss: 0.0044\n",
      "Epoch [162/200], Step [5/9], Loss: 0.0057\n",
      "Epoch [162/200], Step [6/9], Loss: 0.0048\n",
      "Epoch [162/200], Step [7/9], Loss: 0.0039\n",
      "Epoch [162/200], Step [8/9], Loss: 0.0046\n",
      "Epoch [163/200], Step [1/9], Loss: 0.0061\n",
      "Epoch [163/200], Step [2/9], Loss: 0.0050\n",
      "Epoch [163/200], Step [3/9], Loss: 0.0044\n",
      "Epoch [163/200], Step [4/9], Loss: 0.0057\n",
      "Epoch [163/200], Step [5/9], Loss: 0.0040\n",
      "Epoch [163/200], Step [6/9], Loss: 0.0061\n",
      "Epoch [163/200], Step [7/9], Loss: 0.0051\n",
      "Epoch [163/200], Step [8/9], Loss: 0.0066\n",
      "Epoch [164/200], Step [1/9], Loss: 0.0039\n",
      "Epoch [164/200], Step [2/9], Loss: 0.0039\n",
      "Epoch [164/200], Step [3/9], Loss: 0.0040\n",
      "Epoch [164/200], Step [4/9], Loss: 0.0057\n",
      "Epoch [164/200], Step [5/9], Loss: 0.0044\n",
      "Epoch [164/200], Step [6/9], Loss: 0.0065\n",
      "Epoch [164/200], Step [7/9], Loss: 0.0045\n",
      "Epoch [164/200], Step [8/9], Loss: 0.0040\n",
      "Epoch [165/200], Step [1/9], Loss: 0.0044\n",
      "Epoch [165/200], Step [2/9], Loss: 0.0040\n",
      "Epoch [165/200], Step [3/9], Loss: 0.0042\n",
      "Epoch [165/200], Step [4/9], Loss: 0.0047\n",
      "Epoch [165/200], Step [5/9], Loss: 0.0062\n",
      "Epoch [165/200], Step [6/9], Loss: 0.0062\n",
      "Epoch [165/200], Step [7/9], Loss: 0.0039\n",
      "Epoch [165/200], Step [8/9], Loss: 0.0050\n",
      "Epoch [166/200], Step [1/9], Loss: 0.0053\n",
      "Epoch [166/200], Step [2/9], Loss: 0.0064\n",
      "Epoch [166/200], Step [3/9], Loss: 0.0045\n",
      "Epoch [166/200], Step [4/9], Loss: 0.0043\n",
      "Epoch [166/200], Step [5/9], Loss: 0.0054\n",
      "Epoch [166/200], Step [6/9], Loss: 0.0049\n",
      "Epoch [166/200], Step [7/9], Loss: 0.0038\n",
      "Epoch [166/200], Step [8/9], Loss: 0.0046\n",
      "Epoch [167/200], Step [1/9], Loss: 0.0040\n",
      "Epoch [167/200], Step [2/9], Loss: 0.0057\n",
      "Epoch [167/200], Step [3/9], Loss: 0.0058\n",
      "Epoch [167/200], Step [4/9], Loss: 0.0040\n",
      "Epoch [167/200], Step [5/9], Loss: 0.0053\n",
      "Epoch [167/200], Step [6/9], Loss: 0.0038\n",
      "Epoch [167/200], Step [7/9], Loss: 0.0056\n",
      "Epoch [167/200], Step [8/9], Loss: 0.0043\n",
      "Epoch [168/200], Step [1/9], Loss: 0.0068\n",
      "Epoch [168/200], Step [2/9], Loss: 0.0053\n",
      "Epoch [168/200], Step [3/9], Loss: 0.0053\n",
      "Epoch [168/200], Step [4/9], Loss: 0.0068\n",
      "Epoch [168/200], Step [5/9], Loss: 0.0038\n",
      "Epoch [168/200], Step [6/9], Loss: 0.0049\n",
      "Epoch [168/200], Step [7/9], Loss: 0.0038\n",
      "Epoch [168/200], Step [8/9], Loss: 0.0053\n",
      "Epoch [169/200], Step [1/9], Loss: 0.0044\n",
      "Epoch [169/200], Step [2/9], Loss: 0.0043\n",
      "Epoch [169/200], Step [3/9], Loss: 0.0044\n",
      "Epoch [169/200], Step [4/9], Loss: 0.0066\n",
      "Epoch [169/200], Step [5/9], Loss: 0.0043\n",
      "Epoch [169/200], Step [6/9], Loss: 0.0038\n",
      "Epoch [169/200], Step [7/9], Loss: 0.0059\n",
      "Epoch [169/200], Step [8/9], Loss: 0.0066\n",
      "Epoch [170/200], Step [1/9], Loss: 0.0053\n",
      "Epoch [170/200], Step [2/9], Loss: 0.0050\n",
      "Epoch [170/200], Step [3/9], Loss: 0.0051\n",
      "Epoch [170/200], Step [4/9], Loss: 0.0043\n",
      "Epoch [170/200], Step [5/9], Loss: 0.0056\n",
      "Epoch [170/200], Step [6/9], Loss: 0.0063\n",
      "Epoch [170/200], Step [7/9], Loss: 0.0053\n",
      "Epoch [170/200], Step [8/9], Loss: 0.0053\n",
      "Epoch [171/200], Step [1/9], Loss: 0.0053\n",
      "Epoch [171/200], Step [2/9], Loss: 0.0073\n",
      "Epoch [171/200], Step [3/9], Loss: 0.0044\n",
      "Epoch [171/200], Step [4/9], Loss: 0.0048\n",
      "Epoch [171/200], Step [5/9], Loss: 0.0067\n",
      "Epoch [171/200], Step [6/9], Loss: 0.0050\n",
      "Epoch [171/200], Step [7/9], Loss: 0.0043\n",
      "Epoch [171/200], Step [8/9], Loss: 0.0044\n",
      "Epoch [172/200], Step [1/9], Loss: 0.0043\n",
      "Epoch [172/200], Step [2/9], Loss: 0.0039\n",
      "Epoch [172/200], Step [3/9], Loss: 0.0041\n",
      "Epoch [172/200], Step [4/9], Loss: 0.0073\n",
      "Epoch [172/200], Step [5/9], Loss: 0.0044\n",
      "Epoch [172/200], Step [6/9], Loss: 0.0065\n",
      "Epoch [172/200], Step [7/9], Loss: 0.0047\n",
      "Epoch [172/200], Step [8/9], Loss: 0.0078\n",
      "Epoch [173/200], Step [1/9], Loss: 0.0039\n",
      "Epoch [173/200], Step [2/9], Loss: 0.0050\n",
      "Epoch [173/200], Step [3/9], Loss: 0.0039\n",
      "Epoch [173/200], Step [4/9], Loss: 0.0060\n",
      "Epoch [173/200], Step [5/9], Loss: 0.0044\n",
      "Epoch [173/200], Step [6/9], Loss: 0.0063\n",
      "Epoch [173/200], Step [7/9], Loss: 0.0061\n",
      "Epoch [173/200], Step [8/9], Loss: 0.0056\n",
      "Epoch [174/200], Step [1/9], Loss: 0.0050\n",
      "Epoch [174/200], Step [2/9], Loss: 0.0045\n",
      "Epoch [174/200], Step [3/9], Loss: 0.0043\n",
      "Epoch [174/200], Step [4/9], Loss: 0.0038\n",
      "Epoch [174/200], Step [5/9], Loss: 0.0059\n",
      "Epoch [174/200], Step [6/9], Loss: 0.0060\n",
      "Epoch [174/200], Step [7/9], Loss: 0.0037\n",
      "Epoch [174/200], Step [8/9], Loss: 0.0042\n",
      "Epoch [175/200], Step [1/9], Loss: 0.0041\n",
      "Epoch [175/200], Step [2/9], Loss: 0.0037\n",
      "Epoch [175/200], Step [3/9], Loss: 0.0042\n",
      "Epoch [175/200], Step [4/9], Loss: 0.0046\n",
      "Epoch [175/200], Step [5/9], Loss: 0.0044\n",
      "Epoch [175/200], Step [6/9], Loss: 0.0042\n",
      "Epoch [175/200], Step [7/9], Loss: 0.0038\n",
      "Epoch [175/200], Step [8/9], Loss: 0.0040\n",
      "Epoch [176/200], Step [1/9], Loss: 0.0040\n",
      "Epoch [176/200], Step [2/9], Loss: 0.0044\n",
      "Epoch [176/200], Step [3/9], Loss: 0.0041\n",
      "Epoch [176/200], Step [4/9], Loss: 0.0037\n",
      "Epoch [176/200], Step [5/9], Loss: 0.0044\n",
      "Epoch [176/200], Step [6/9], Loss: 0.0061\n",
      "Epoch [176/200], Step [7/9], Loss: 0.0039\n",
      "Epoch [176/200], Step [8/9], Loss: 0.0046\n",
      "Epoch [177/200], Step [1/9], Loss: 0.0049\n",
      "Epoch [177/200], Step [2/9], Loss: 0.0043\n",
      "Epoch [177/200], Step [3/9], Loss: 0.0060\n",
      "Epoch [177/200], Step [4/9], Loss: 0.0044\n",
      "Epoch [177/200], Step [5/9], Loss: 0.0038\n",
      "Epoch [177/200], Step [6/9], Loss: 0.0038\n",
      "Epoch [177/200], Step [7/9], Loss: 0.0054\n",
      "Epoch [177/200], Step [8/9], Loss: 0.0059\n",
      "Epoch [178/200], Step [1/9], Loss: 0.0062\n",
      "Epoch [178/200], Step [2/9], Loss: 0.0038\n",
      "Epoch [178/200], Step [3/9], Loss: 0.0059\n",
      "Epoch [178/200], Step [4/9], Loss: 0.0045\n",
      "Epoch [178/200], Step [5/9], Loss: 0.0044\n",
      "Epoch [178/200], Step [6/9], Loss: 0.0037\n",
      "Epoch [178/200], Step [7/9], Loss: 0.0043\n",
      "Epoch [178/200], Step [8/9], Loss: 0.0042\n",
      "Epoch [179/200], Step [1/9], Loss: 0.0042\n",
      "Epoch [179/200], Step [2/9], Loss: 0.0042\n",
      "Epoch [179/200], Step [3/9], Loss: 0.0053\n",
      "Epoch [179/200], Step [4/9], Loss: 0.0040\n",
      "Epoch [179/200], Step [5/9], Loss: 0.0037\n",
      "Epoch [179/200], Step [6/9], Loss: 0.0038\n",
      "Epoch [179/200], Step [7/9], Loss: 0.0037\n",
      "Epoch [179/200], Step [8/9], Loss: 0.0067\n",
      "Epoch [180/200], Step [1/9], Loss: 0.0044\n",
      "Epoch [180/200], Step [2/9], Loss: 0.0059\n",
      "Epoch [180/200], Step [3/9], Loss: 0.0062\n",
      "Epoch [180/200], Step [4/9], Loss: 0.0045\n",
      "Epoch [180/200], Step [5/9], Loss: 0.0050\n",
      "Epoch [180/200], Step [6/9], Loss: 0.0044\n",
      "Epoch [180/200], Step [7/9], Loss: 0.0065\n",
      "Epoch [180/200], Step [8/9], Loss: 0.0056\n",
      "Epoch [181/200], Step [1/9], Loss: 0.0043\n",
      "Epoch [181/200], Step [2/9], Loss: 0.0058\n",
      "Epoch [181/200], Step [3/9], Loss: 0.0044\n",
      "Epoch [181/200], Step [4/9], Loss: 0.0069\n",
      "Epoch [181/200], Step [5/9], Loss: 0.0045\n",
      "Epoch [181/200], Step [6/9], Loss: 0.0059\n",
      "Epoch [181/200], Step [7/9], Loss: 0.0052\n",
      "Epoch [181/200], Step [8/9], Loss: 0.0042\n",
      "Epoch [182/200], Step [1/9], Loss: 0.0053\n",
      "Epoch [182/200], Step [2/9], Loss: 0.0058\n",
      "Epoch [182/200], Step [3/9], Loss: 0.0036\n",
      "Epoch [182/200], Step [4/9], Loss: 0.0042\n",
      "Epoch [182/200], Step [5/9], Loss: 0.0051\n",
      "Epoch [182/200], Step [6/9], Loss: 0.0037\n",
      "Epoch [182/200], Step [7/9], Loss: 0.0045\n",
      "Epoch [182/200], Step [8/9], Loss: 0.0051\n",
      "Epoch [183/200], Step [1/9], Loss: 0.0057\n",
      "Epoch [183/200], Step [2/9], Loss: 0.0051\n",
      "Epoch [183/200], Step [3/9], Loss: 0.0039\n",
      "Epoch [183/200], Step [4/9], Loss: 0.0066\n",
      "Epoch [183/200], Step [5/9], Loss: 0.0042\n",
      "Epoch [183/200], Step [6/9], Loss: 0.0040\n",
      "Epoch [183/200], Step [7/9], Loss: 0.0038\n",
      "Epoch [183/200], Step [8/9], Loss: 0.0041\n",
      "Epoch [184/200], Step [1/9], Loss: 0.0044\n",
      "Epoch [184/200], Step [2/9], Loss: 0.0082\n",
      "Epoch [184/200], Step [3/9], Loss: 0.0043\n",
      "Epoch [184/200], Step [4/9], Loss: 0.0047\n",
      "Epoch [184/200], Step [5/9], Loss: 0.0038\n",
      "Epoch [184/200], Step [6/9], Loss: 0.0042\n",
      "Epoch [184/200], Step [7/9], Loss: 0.0044\n",
      "Epoch [184/200], Step [8/9], Loss: 0.0039\n",
      "Epoch [185/200], Step [1/9], Loss: 0.0064\n",
      "Epoch [185/200], Step [2/9], Loss: 0.0037\n",
      "Epoch [185/200], Step [3/9], Loss: 0.0039\n",
      "Epoch [185/200], Step [4/9], Loss: 0.0063\n",
      "Epoch [185/200], Step [5/9], Loss: 0.0052\n",
      "Epoch [185/200], Step [6/9], Loss: 0.0065\n",
      "Epoch [185/200], Step [7/9], Loss: 0.0067\n",
      "Epoch [185/200], Step [8/9], Loss: 0.0045\n",
      "Epoch [186/200], Step [1/9], Loss: 0.0044\n",
      "Epoch [186/200], Step [2/9], Loss: 0.0039\n",
      "Epoch [186/200], Step [3/9], Loss: 0.0039\n",
      "Epoch [186/200], Step [4/9], Loss: 0.0065\n",
      "Epoch [186/200], Step [5/9], Loss: 0.0058\n",
      "Epoch [186/200], Step [6/9], Loss: 0.0052\n",
      "Epoch [186/200], Step [7/9], Loss: 0.0043\n",
      "Epoch [186/200], Step [8/9], Loss: 0.0040\n",
      "Epoch [187/200], Step [1/9], Loss: 0.0064\n",
      "Epoch [187/200], Step [2/9], Loss: 0.0062\n",
      "Epoch [187/200], Step [3/9], Loss: 0.0044\n",
      "Epoch [187/200], Step [4/9], Loss: 0.0038\n",
      "Epoch [187/200], Step [5/9], Loss: 0.0042\n",
      "Epoch [187/200], Step [6/9], Loss: 0.0042\n",
      "Epoch [187/200], Step [7/9], Loss: 0.0067\n",
      "Epoch [187/200], Step [8/9], Loss: 0.0053\n",
      "Epoch [188/200], Step [1/9], Loss: 0.0043\n",
      "Epoch [188/200], Step [2/9], Loss: 0.0051\n",
      "Epoch [188/200], Step [3/9], Loss: 0.0040\n",
      "Epoch [188/200], Step [4/9], Loss: 0.0060\n",
      "Epoch [188/200], Step [5/9], Loss: 0.0062\n",
      "Epoch [188/200], Step [6/9], Loss: 0.0038\n",
      "Epoch [188/200], Step [7/9], Loss: 0.0036\n",
      "Epoch [188/200], Step [8/9], Loss: 0.0061\n",
      "Epoch [189/200], Step [1/9], Loss: 0.0043\n",
      "Epoch [189/200], Step [2/9], Loss: 0.0054\n",
      "Epoch [189/200], Step [3/9], Loss: 0.0048\n",
      "Epoch [189/200], Step [4/9], Loss: 0.0037\n",
      "Epoch [189/200], Step [5/9], Loss: 0.0058\n",
      "Epoch [189/200], Step [6/9], Loss: 0.0058\n",
      "Epoch [189/200], Step [7/9], Loss: 0.0066\n",
      "Epoch [189/200], Step [8/9], Loss: 0.0042\n",
      "Epoch [190/200], Step [1/9], Loss: 0.0047\n",
      "Epoch [190/200], Step [2/9], Loss: 0.0039\n",
      "Epoch [190/200], Step [3/9], Loss: 0.0060\n",
      "Epoch [190/200], Step [4/9], Loss: 0.0037\n",
      "Epoch [190/200], Step [5/9], Loss: 0.0038\n",
      "Epoch [190/200], Step [6/9], Loss: 0.0039\n",
      "Epoch [190/200], Step [7/9], Loss: 0.0048\n",
      "Epoch [190/200], Step [8/9], Loss: 0.0046\n",
      "Epoch [191/200], Step [1/9], Loss: 0.0044\n",
      "Epoch [191/200], Step [2/9], Loss: 0.0062\n",
      "Epoch [191/200], Step [3/9], Loss: 0.0036\n",
      "Epoch [191/200], Step [4/9], Loss: 0.0059\n",
      "Epoch [191/200], Step [5/9], Loss: 0.0036\n",
      "Epoch [191/200], Step [6/9], Loss: 0.0037\n",
      "Epoch [191/200], Step [7/9], Loss: 0.0041\n",
      "Epoch [191/200], Step [8/9], Loss: 0.0045\n",
      "Epoch [192/200], Step [1/9], Loss: 0.0043\n",
      "Epoch [192/200], Step [2/9], Loss: 0.0056\n",
      "Epoch [192/200], Step [3/9], Loss: 0.0074\n",
      "Epoch [192/200], Step [4/9], Loss: 0.0046\n",
      "Epoch [192/200], Step [5/9], Loss: 0.0061\n",
      "Epoch [192/200], Step [6/9], Loss: 0.0044\n",
      "Epoch [192/200], Step [7/9], Loss: 0.0043\n",
      "Epoch [192/200], Step [8/9], Loss: 0.0038\n",
      "Epoch [193/200], Step [1/9], Loss: 0.0046\n",
      "Epoch [193/200], Step [2/9], Loss: 0.0047\n",
      "Epoch [193/200], Step [3/9], Loss: 0.0060\n",
      "Epoch [193/200], Step [4/9], Loss: 0.0042\n",
      "Epoch [193/200], Step [5/9], Loss: 0.0042\n",
      "Epoch [193/200], Step [6/9], Loss: 0.0085\n",
      "Epoch [193/200], Step [7/9], Loss: 0.0055\n",
      "Epoch [193/200], Step [8/9], Loss: 0.0043\n",
      "Epoch [194/200], Step [1/9], Loss: 0.0062\n",
      "Epoch [194/200], Step [2/9], Loss: 0.0065\n",
      "Epoch [194/200], Step [3/9], Loss: 0.0046\n",
      "Epoch [194/200], Step [4/9], Loss: 0.0041\n",
      "Epoch [194/200], Step [5/9], Loss: 0.0044\n",
      "Epoch [194/200], Step [6/9], Loss: 0.0043\n",
      "Epoch [194/200], Step [7/9], Loss: 0.0039\n",
      "Epoch [194/200], Step [8/9], Loss: 0.0039\n",
      "Epoch [195/200], Step [1/9], Loss: 0.0044\n",
      "Epoch [195/200], Step [2/9], Loss: 0.0043\n",
      "Epoch [195/200], Step [3/9], Loss: 0.0045\n",
      "Epoch [195/200], Step [4/9], Loss: 0.0061\n",
      "Epoch [195/200], Step [5/9], Loss: 0.0053\n",
      "Epoch [195/200], Step [6/9], Loss: 0.0061\n",
      "Epoch [195/200], Step [7/9], Loss: 0.0061\n",
      "Epoch [195/200], Step [8/9], Loss: 0.0044\n",
      "Epoch [196/200], Step [1/9], Loss: 0.0062\n",
      "Epoch [196/200], Step [2/9], Loss: 0.0039\n",
      "Epoch [196/200], Step [3/9], Loss: 0.0061\n",
      "Epoch [196/200], Step [4/9], Loss: 0.0044\n",
      "Epoch [196/200], Step [5/9], Loss: 0.0054\n",
      "Epoch [196/200], Step [6/9], Loss: 0.0040\n",
      "Epoch [196/200], Step [7/9], Loss: 0.0053\n",
      "Epoch [196/200], Step [8/9], Loss: 0.0059\n",
      "Epoch [197/200], Step [1/9], Loss: 0.0045\n",
      "Epoch [197/200], Step [2/9], Loss: 0.0048\n",
      "Epoch [197/200], Step [3/9], Loss: 0.0038\n",
      "Epoch [197/200], Step [4/9], Loss: 0.0053\n",
      "Epoch [197/200], Step [5/9], Loss: 0.0053\n",
      "Epoch [197/200], Step [6/9], Loss: 0.0054\n",
      "Epoch [197/200], Step [7/9], Loss: 0.0037\n",
      "Epoch [197/200], Step [8/9], Loss: 0.0037\n",
      "Epoch [198/200], Step [1/9], Loss: 0.0040\n",
      "Epoch [198/200], Step [2/9], Loss: 0.0039\n",
      "Epoch [198/200], Step [3/9], Loss: 0.0065\n",
      "Epoch [198/200], Step [4/9], Loss: 0.0037\n",
      "Epoch [198/200], Step [5/9], Loss: 0.0044\n",
      "Epoch [198/200], Step [6/9], Loss: 0.0036\n",
      "Epoch [198/200], Step [7/9], Loss: 0.0038\n",
      "Epoch [198/200], Step [8/9], Loss: 0.0036\n",
      "Epoch [199/200], Step [1/9], Loss: 0.0043\n",
      "Epoch [199/200], Step [2/9], Loss: 0.0043\n",
      "Epoch [199/200], Step [3/9], Loss: 0.0043\n",
      "Epoch [199/200], Step [4/9], Loss: 0.0053\n",
      "Epoch [199/200], Step [5/9], Loss: 0.0055\n",
      "Epoch [199/200], Step [6/9], Loss: 0.0064\n",
      "Epoch [199/200], Step [7/9], Loss: 0.0039\n",
      "Epoch [199/200], Step [8/9], Loss: 0.0044\n",
      "Epoch [200/200], Step [1/9], Loss: 0.0057\n",
      "Epoch [200/200], Step [2/9], Loss: 0.0037\n",
      "Epoch [200/200], Step [3/9], Loss: 0.0056\n",
      "Epoch [200/200], Step [4/9], Loss: 0.0072\n",
      "Epoch [200/200], Step [5/9], Loss: 0.0064\n",
      "Epoch [200/200], Step [6/9], Loss: 0.0042\n",
      "Epoch [200/200], Step [7/9], Loss: 0.0039\n",
      "Epoch [200/200], Step [8/9], Loss: 0.0059\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# 设置设备，优先使用 GPU（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 实例化简化后的 BYOL 模型，hidden_layer=-2 表示取倒数第二层的输出作为隐藏表示\n",
    "model = BYOL(net=base_net, hidden_layer=-2, projection_size=3, projection_hidden_size=32).to(device)\n",
    "model.train()\n",
    "\n",
    "# 超参数设置\n",
    "epochs = 200\n",
    "batch_size = 4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, _) in enumerate(train_dataloader):\n",
    "        if data.size(0) < 2:\n",
    "            continue\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播：model(data) 返回 (projection, representation)\n",
    "        projection, representation = model(data)\n",
    "        \n",
    "        # 此处定义一个简单的损失函数：投影输出的 L2 范数\n",
    "        # 仅作为示例，实际任务中可根据需要设计损失函数\n",
    "        loss = projection.norm()\n",
    "        \n",
    "        # 反向传播和参数更新\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Step [{batch_idx+1}/{len(train_dataloader)}], Loss: {running_loss/100:.4f}')\n",
    "        running_loss = 0.0\n",
    "        # Save the model checkpoint\n",
    "torch.save(model.state_dict(), f'byol_model.pth')\n",
    "\n",
    "print('Training completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n",
      "Accuracy: 0.2424\n",
      "Recall: 0.2424\n",
      "F1 Score: 0.1564\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Disturbance       0.00      0.00      0.00        13\n",
      "          IF       0.38      0.80      0.52        10\n",
      "      Normal       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.24        33\n",
      "   macro avg       0.13      0.27      0.17        33\n",
      "weighted avg       0.12      0.24      0.16        33\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangyu/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/xiangyu/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/xiangyu/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Test the model's performance\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"Testing the model...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_dataloader:  # Using test_dataloader instead of train_dataloader\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Extract features using the online encoder\n",
    "        # BYOL model returns both projection and representation, but we need representation\n",
    "        projection, representation = model(images)\n",
    "        \n",
    "        # Use the last layer output as logits for classification\n",
    "        preds = torch.argmax(projection, dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=dataset.classes))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=dataset.classes, \n",
    "            yticklabels=dataset.classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 2, 1, 0, 0, 1, 0, 2, 2, 0, 2, 0, 0, 2, 1, 1, 0, 0, 1, 0, 2, 2,\n",
       "        1, 1, 1, 1, 2, 0, 2, 0, 1, 2, 0]),\n",
       " array([1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1,\n",
       "        1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels, all_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
